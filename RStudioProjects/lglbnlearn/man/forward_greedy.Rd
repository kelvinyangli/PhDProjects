% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/forward_greedy.R
\name{forward_greedy}
\alias{forward_greedy}
\title{A forward greedy search function}
\usage{
forward_greedy(data, arities, vars, sampleSize, target, model, base = exp(1),
  sigma = 3, dataNumeric = NULL, varCnt = NULL, targetAdptProbs = NULL,
  debug = FALSE)
}
\arguments{
\item{data}{A categorical data set.}

\item{arities}{A vector of variable arities in data, in the same order as the column names of data.}

\item{vars}{A vector of all variables in data, in the same order as the column names of data.}

\item{sampleSize}{The sample size. That is, the number of rows of data.}

\item{target}{The target node, whose Markov blanket we are interested in.}

\item{model}{The options are cpt, logit (binary only), naive bayes and random models.}

\item{base}{The base of logarithm. The default is the natural log.}

\item{sigma}{The standard derivation of the assumed Gaussian distribution for parameter prior. The 
default value is 3 as suggested by the original paper.}

\item{dataNumeric}{This parameter is for mml_logit. The numeric format of the given data set. 
Variable values start from 0.}

\item{varCnt}{This parameter is for mml_cpt. As explained by argument name. 
It is obtained by getting the detailed information of the given data using the function 
count_occurance().}

\item{targetAdptProbs}{This parameter is for mml_random. A matrix that stores the target's probability
for each of its value and each data point. It is a arity(target) by n matrix.}

\item{debug}{A boolean argument to show the detailed Markov blanket inclusion steps based on each 
mml score.}
}
\value{
The function returns the learned Markov blanket candidates according to the assigned objective 
function.
}
\description{
This is a forward greedy seearch function that can be used incorprate with an objective function mml
cpt, logit, or naive bayes (adaptive code) to discovery Markov blanket candidates. It is a greedy search, so the 
function will stop if there is no better option to add into the current Markov blanket.
}
