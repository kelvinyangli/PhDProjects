% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/forward_greedy.R
\name{forward_greedy}
\alias{forward_greedy}
\title{A forward greedy search function}
\usage{
forward_greedy(data, arities, vars, sampleSize, target, score, base = exp(1),
  sigma = 3, dataNumeric = NULL, indexListPerNodePerValue = NULL,
  probSign = NULL, debug = FALSE)
}
\arguments{
\item{data}{A categorical data set.}

\item{arities}{A vector of variable arities in data, in the same order as the column names of data.}

\item{vars}{A vector of all variables in data, in the same order as the column names of data.}

\item{sampleSize}{The sample size. That is, the number of rows of data.}

\item{target}{The target node, whose Markov blanket we are interested in.}

\item{score}{The objective function to pass to. Curernt choices are mml_cpt, mml_logit and mml_nb.}

\item{base}{The base of logarithm. The default is the natural log.}

\item{sigma}{The standard derivation of the assumed Gaussian distribution for parameter prior. The 
default value is 3 as suggested by the original paper.}

\item{dataNumeric}{This parameter is for mml_logit. The numeric format of the given data set. 
Variable values start from 0.}

\item{indexListPerNodePerValue}{This parameter is for mml_cpt. As explained by argument name. 
It is obtained by getting the detailed information of the given data using the function 
count_occurance().}

\item{probSign}{This parameter is for mml_nb. A data frame with 1 and -1, which corresponds to the 
1st and 2nd level of a varaible.}

\item{debug}{A boolean argument to show the detailed Markov blanket inclusion steps based on each 
mml score.}
}
\value{
The function returns the learned Markov blanket candidates according to the assigned objective 
function.
}
\description{
This is a forward greedy seearch function that can be used incorprate with an objective function mml
cpt, logit, or naive bayes to discovery Markov blanket candidates. It is a greedy search, so the 
function will stop if there is no better option to add into the current Markov blanket.
}
