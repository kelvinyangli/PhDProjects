---
title: "MB_learner_comparison"
author: "Kelvin Li"
date: "21 July 2017"
output: html_document
---

```{r global_options, include=F, warning=F, eval=F, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = F, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))
```

## Random model
```{r}
# dag = empty.graph(c("Y", paste0("X", 1:7)))
# for (i in 2:nnodes(dag)) {
#   dag = set.arc(dag, nodes(dag)[1], nodes(dag)[i])
# }
dag = randDag(8, 2)
#graphviz.plot(dag)
cpts = randCPTs(dag, 2, 1)
sampleSize = n = 1000
data = rbn(cpts, n)
vars = colnames(data)
arities = sapply(data, nlevels)
names(arities) = c()
# y = "V7"
# x = bnlearn::mb(dag, y)
# yIndex = which(vars == y)
# xIndices = which(vars %in% x)
# mb(dag, y)
```

## Executing MML_NB
```{r, eval = T}
#probSign = get_prob_sign(data)
lst_nb = list() 
for (i in 1:length(vars)) {
  #i=4
  y = vars[i]
  #forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb_adaptive, debug = T)
  lst_nb[[i]] = forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb_adaptive, debug = F)
  #forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb, probSign = probSign, debug = T)
}

# Rprof("mml_nb.out")
# forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb_adaptive, debug = F)
# Rprof(NULL)
# proftable("mml_nb.out")


```

## MML_CPT
```{r}
indexListPerNodePerValue = count_occurance(data, arities)
lst_cpt = list()
for (i in 1:length(vars)) {
  y = vars[i]
  #forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, y, base = exp(1), debug =T)
  lst_cpt[[i]] = forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, y, base = exp(1), debug = F)
  #forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, vars[1], base = exp(1), debug = T)
}
```
The above mml + cpt is even smaller than mml + nb, which is expected to be shorter due to less number of parameters comparing with a full cpt!!!

## MML_Logit

```{r, eval = F}
dataNumeric = factor2numeric(data)
vars = names(data)
sampleSize = n
lst_logit = list()
for (i in 1:length(vars)) {
  y = vars[i]
  lst_logit[[i]] = forward_greedy(data, arities, vars, sampleSize, y, score = mml_logit, dataNumeric = dataNumeric, debug = F)
  #forward_greedy(data, arities, vars, sampleSize, vars[1], score = mml_logit, dataNumeric = dataNumeric, debug = T)
}
```

## Accuracy
```{r}
acc_nb = acc_cpt = acc_logit = matrix(0, nrow = length(vars), ncol = 2)
for (i in 1:length(vars)) {
  acc_nb[i, ] = classification_accuracy_mb(mb(dag, vars[i]), lst_nb[[i]], vars, vars[i])
}

for (i in 1:length(vars)) {
  acc_cpt[i, ] = classification_accuracy_mb(mb(dag, vars[i]), lst_cpt[[i]], vars, vars[i])  
}

# for (i in 1:length(vars)) {
#   acc_logit[i, ] = classification_accuracy_mb(mb(dag, vars[i]), lst_logit[[i]], vars, vars[i])  
# }

x1 = round(colMeans(acc_nb), 2)
x2 = round(1.96 * apply(acc_nb, 2, sd) / sqrt(length(vars)), 2)
x3 = round(colMeans(acc_cpt), 2)
x4 = round(1.96 * apply(acc_cpt, 2, sd) / sqrt(length(vars)), 2)
# x5 = round(colMeans(acc_logit), 2)
# x6 = round(1.96 * apply(acc_logit, 2, sd) / sqrt(length(vars)), 2)

cat("mml_nb:", x1[1], "(", x2[1], ")", x1[2], "(", x2[2], ") \n")
cat("mml_cpt:", x3[1], "(", x4[1], ")", x3[2], "(", x4[2], ") \n")
# cat("mml_logit:", x5[1], "(", x6[1], ")", x5[2], "(", x6[2], ") \n")
```

## Real models
Used a different log factorial estimation, so the results could be different from when previous UAI results
```{r}
score = "mml_nb"
model = "insurance3"
sampleSize = 500
datasets = list.files(paste0("~/Documents/Experiments/UAI_exp/", model, "/data_csv/", sampleSize, "/"))
dag = readRDS(paste0("~/Documents/Experiments/UAI_exp/", model, "/dag/", model, ".rds"))
vars = bnlearn::nodes(dag)
```

```{r, eval=F}
for (i in 1:length(datasets)) {
  data = read.csv(paste0("~/Documents/Experiments/UAI_exp/", model, "/data_csv/", sampleSize, "/", datasets[i]))  
  data = numeric2categorical(data)
  arities = sapply(data, nlevels)
  names(arities) = c()
  if (score == "mml_cpt") indexListPerNodePerValue = count_occurance(data, arities)
  lst_learned = list() 
  for (j in 1:length(vars)) {
    y = vars[j]
    if (score == "mml_nb") {
      lst_learned[[j]] = forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb_adaptive)
    } else if (score == "mml_cpt") {
      lst_learned[[j]] = forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, y, base = exp(1))
    }
  }
  lst_learned = symmetry_correction(vars, lst_learned, "AND")
  filename = strsplit(datasets[i], ".csv")[[1]]
  saveRDS(lst_learned, paste0("~/Documents/Experiments/", score, "/", model, "/mb_learned/", sampleSize, "/", filename, ".rds"))
}
```

```{r}
#model = "insurance"
score = "mml_cpt"
#sampleSize = 500
mb_true = list()
for (i in 1:length(vars)) {
  mb_true[[i]] = bnlearn::mb(dag, vars[i])
}
files = list.files(paste0("~/Documents/Experiments/", score, "/", model, "/mb_learned/", sampleSize))
#files = list.files("~/Documents/Experiments/UAI_exp/insurance3/mb/500/")
ed = rep(0, length(files)) 
for (i in 1:length(files)) {
  mb_learned = readRDS(paste0("~/Documents/Experiments/", score, "/", model, "/mb_learned/", sampleSize, "/", files[i]))
  #mb_learned = readRDS(paste0("~/Documents/Experiments/UAI_exp/insurance3/mb/500/", files[i]))
  ee = c()
  for (j in 1:length(vars)) {
    y = vars[j]
    #ee = c(ee, edit_dist_mb(mb_true[[j]], mb_learned[[j]], vars, y))
    ed[i] = ed[i] + edit_dist_mb(mb_true[[j]], mb_learned[[j]], vars, y)
  }
}

c = 1.96 * sd(ed) / sqrt(length(ed))
cat(mean(ed), "+/-", c)
```

```{r}
248 240 256 226 264 266 248 254 300 238 cpt_current
230 234 232 228 234 236 240 242 248 226 cpt_previous

```






