---
title: "kdd_mb_exp"
author: "kl"
date: "31 January 2018"
output: html_document
---

```{r global_options, include=F, warning=F, eval=F, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = F, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))

```

These exp take 5 parameters into account, ordered by priority: 

* nvars 10, 30, 50, 100
* n 50, 500, 5000
* npa 2, 4, 6
* narity 2, 3, 4
* beta (derichlet concentration parameter) 1, 5, 25

```{r}
dir = "~/Documents/Experiments/kdd_exp/"
vnvars = c(10, 50, 100)
vnpa = c(2, 4, 6)
vnarity = c(2, 3, 4)
vbeta = c(1, 5, 25)
vn = c(50, 500, 5000)
for (nvars in vnvars) {
  
  for (npa in vnpa) {
          
    for (narity in vnarity) {
      
      for (beta in vbeta) {
        nvars = 50
        npa = narity = 2 
        beta = 1
        dirname = paste(nvars, npa, narity, beta, sep = "_")
        
        for (itr in 1:5) {
  
          sd = randSeed()
          set.seed(sd)
          dag = randDag(nvars, npa)
          cpts = randCPTs(dag, narity, beta)
          nodes = bnlearn::nodes(dag)
          mbtrue = list()
          i = 1
          for (x in nodes) {
            mbtrue[[i]] = bnlearn::mb(dag, x)
            i = i + 1
          }
          saveRDS(dag, paste0(dir, dirname, "/dag/dag_", sd, ".rds"))
          saveRDS(cpts, paste0(dir, dirname, "/cpts/cpt_", sd, ".rds"))
          saveRDS(mbtrue, paste0(dir, dirname, "/mb_true/mb_", sd, ".rds"))
          
          for (n in vn) {
            
            datasd = randSeed()
            set.seed(datasd)
            data = rbn(cpts, n)  
            saveRDS(data, paste0(dir, dirname, "/data_rds/data_", n, "_", sd, "_", datasd, ".rds"))
            write.csv(data, paste0(dir, dirname, "/data_csv/data_", n, "_", sd, "_", datasd, ".csv"), row.names = F)
            
          }
        }# end itr
      }# end beta
    }
  }
}

```


## learning
```{r}
# library(parallel)
# library(foreach)
# library(doParallel)
dir = "~/Documents/Experiments/kdd_exp/"
params = "30_5_4_1"
datasets = list.files(paste0(dir, params, "/data_rds/"))
#dags = list.files(paste0(dir, params, "/dag/"))
for (i in 36:50) {
  cat(i, "\n")
  data = readRDS(paste0(dir, params, "/data_rds/", datasets[i]))
  #dag = readRDS(paste0(dir, params, "/dag/", dags[i]))
  filename = paste0(strsplit(datasets[i], "_")[[1]][-1], collapse = "_")
  nvars = ncol(data)
  vars = names(data)
  n = nrow(data)
  arities = sapply(data, nlevels)
  names(arities) = c()
  varCnt = count_occurance(data, arities)
  mbcpt = mbnb = mbrand = list()
  
  mbiamb = lapply(vars, learn.mb, x = data, method = "iamb")
  saveRDS(mbiamb, paste0(dir, params, "/iamb/iamb_", filename))
  # mbinter = lapply(vars, learn.mb, x = data, method = "inter.iamb")
  # saveRDS(mbinter, paste0(dir, params, "/inter_iamb/inter_", filename))
  # 
  data = data.matrix(data)
  
  # for (target in vars) {
  #   cat(target, "\n")
  #   forward_greedy(data, arities, vars, n, target, "random", varCnt = varCnt, prior = "uniform")
  # }
  
  ## starting parallel computing ###
  no_cores = 3
  registerDoParallel(no_cores)

  mbrand = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar%
    forward_greedy(data, arities, vars, n, target, "random", varCnt = varCnt, prior = "uniform")

  mbcpt = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar%
    forward_greedy_fast(data, varCnt, arities, vars, n, target)

  mbnb = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar%
    forward_greedy(data, arities, vars, n, target, "nb")

  saveRDS(mbrand, paste0(dir, params, "/rand/rand_", filename))
  saveRDS(mbcpt, paste0(dir, params, "/cpt/cpt_", filename))
  saveRDS(mbnb, paste0(dir, params, "/nb/nb_", filename))
  
}

stopImplicitCluster()
```




## Evaluating
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
model = "30_5_4_1"
#res = c()

true = list.files(paste0(dir, model, "/mb_true"))
m = matrix(0, 15, 10)
m[, 1] = model
amb = c() # average mb size
for (x in true) {
  
  mbt = readRDS(paste0(dir, model, "/mb_true/", x))
  amb = c(amb, round(sum(sapply(mbt, length)) / length(mbt), 2))
  
}
m[, 2] = rep(amb, 3)
m[, 3] = c(rep(500, 5), rep(2000, 5), rep(5000, 5)) 
l = 4

  
for (folder in c("cpt", "nb", "rand", "iamb")) {
  
  for (n in c(500, 2000, 5000)) {

    if (n == 2000) {
      ind = 1
    } else if (n == 5000) {
      ind = 2
    } else if (n == 500) {
      ind = 3
    }
       
    learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
    edList = rep(list(rep(0, 30)), length(learned)) # vector of ed for all true models
    for (i in 1:length(true)) {
      
      mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
      for (j in ((5 * (i - 1)) + 1):(5 * i)) {
        
        mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
        for (k in 1:length(mbt)) edList[[j]][k] = mb_false_finding(mbt[[k]], mbl[[k]])
        
      }# end for j 
      
    }# end for i 
   
  }# end for n
  
}# end for each method

#write.table(m, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)
# line = matrix(c(model, n, res), 1)
# write.table(line, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)
```


## DAG stats
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
model = "10_2_2_1"
mbs = list.files(paste0(dir, model, "/mb_true"))
for (x in mbs) {
  
  mbtrue = readRDS(paste0(dir, model, "/mb_true/", x))
  cat(sum(sapply(mbtrue, length)) / length(mbtrue), "\n")
  
}


```

















