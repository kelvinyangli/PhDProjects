---
title: "mml87_nb"
author: "Kelvin"
date: "27 June 2017"
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=F, warning=F, message=F, eval=F, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = T, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))
```

## MLE of NB parameters
The MLE estimations of NB parameters are as follows:
\[
\begin{aligned}
P(Y=y) &= \frac{count(y)}{n} \\
P(X_j = x|Y = y) &= \frac{count(x) \cup count(y)}{count(y)}
\end{aligned}
\]

## Negative log likelihood of NB
For a Naive Bayes model with binary variable, its parameters are $\{P(Y_i=1), P(X_{ij}=1|Y_i=1), P(X_{ij}=1|Y_i=0)\}$. To simply the notations, we use $\{q_{i1}, p_{ij1}, p_{ij0}\}$ to denote the above probabilities respectively, and denote 
\[
\begin{aligned}
\pi_{i1} &= \prod_{j=1}^m p_{ij1}^{x_i}(1-p_{ij1})^{1-x_i} \\
\pi_{i0} &= \prod_{j=1}^m p_{ij0}^{x_i}(1-p_{ij0})^{1-x_i} \\
pxy_{i1} &= q_{i1}\pi_{i1} \\
pxy_{i0} &= (1-q_{i1})\pi_{i0} \\
p(\vec{X_i}) &= px_i = pxy_{i1} + pxy_{i0}
\end{aligned}
\]
The likelihood of Naive Bayes given a data set is then
\[
\begin{aligned}
l = \prod_i^n P(Y_i=1|\vec{X_i})^{Y_i} (1-P(Y_i=1|\vec{X_i}))^{1-Y_i}
\end{aligned}
\]
where the posterior probability of $Y_i$ given a vector $\vec{X_i} = <X_{i1}, \dots, X_{im}>$ is 
\[
\begin{aligned}
P(Y_i=T|\vec{X_i}) &= \frac{P(Y_i=1)\prod_{j=1}^m P(X_{ij}=1|Y_i=1)^{X_i}(1-P(X_{ij}=1|Y_i=1))^{1-X_i}}{P(\vec{X_i})} \\
&= \frac{q_{i1}\pi_{i1}}{px_i}
\end{aligned}
\]
The negative loglikelihood
\[
\begin{aligned}
L &= -\sum_{i=1}^n \left[Y_i\ln p(Y_i=1|\vec{X_i}) + (1-Y_i) \ln (1-p(Y_i=1|\vec{X_i}))\right] \\
&= -\sum_{i=1}^n \left[Y_i \ln q_{i1} + (1-Y_i) \ln (1-q_{i1}) + Y_i \ln \pi_{i1} + (1-y_i) \ln \pi_{i0} - \ln px_i\right]
\end{aligned}
\]

## First derivatives
Before differentiating nll w.r.t. each parameter, we derive the followings first
\[
\begin{aligned}
\frac{\partial}{\partial q_{i1}}px_i &= \pi_{i1} - \pi_{i0} \\
\frac{\partial}{\partial p_{ij1}}\pi_{i1} &= \frac{\pi_{i1}}{p_{ij1}^{x_{ij}}(p_{ij1}-1)^{1-x_{ij}}} \\
\frac{\partial}{\partial p_{ij0}}\pi_{i0} &= \frac{\pi_{i0}}{p_{ij0}^{x_{ij}}(p_{ij0}-1)^{1-x_{ij}}} \\
\frac{\partial}{\partial p_{ij1}}px_i &= \frac{\partial}{\partial p_{ij1}}pxy_{i1} = \frac{pxy_{i1}}{p_{ij1}^{x_{ij}}(p_{ij1}-1)^{1-x_{ij}}} \\
\frac{\partial}{\partial p_{ij0}}px_i &= \frac{\partial}{\partial p_{ij0}}pxy_{i0} = \frac{pxy_{i0}}{p_{ij0}^{x_{ij}}(p_{ij0}-1)^{1-x_{ij}}}
\end{aligned}
\]
The first derivatives of the negative log likelihood w.r.t. each parameter are
\[
\begin{aligned}
\frac{\partial L}{\partial q_{i1}} &= -\sum_{i=1}^n \left[\frac{Y_i}{q_{i1}} - \frac{1-Y_i}{1-q_{i1}} -\frac{\pi_{i1}-\pi_{i0}}{px_i} \right] \\
\frac{\partial L}{\partial p_{ik1}} &= -\sum_{i=1}^n \left[\frac{Y_i}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}} - \frac{pxy_{i1}}{p_{ik1}^{x_{ij}}(p_{ik1}-1)^{1-x_{ij}}px_i}\right] \\
\frac{\partial L}{\partial p_{ik0}} &= -\sum_{i=1}^n \left[\frac{1-Y_i}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}} - \frac{pxy_{i0}}{p_{ik0}^{x_{ij}}(p_{ik0}-1)^{1-x_{ij}}px_i}\right] \\
\end{aligned}
\]

## Second derivatives
Diagonal entries 
\[
\begin{aligned}
\frac{\partial^2L}{\partial q_{i1}^2} &= \sum_{i=1}^n \left[\frac{Y_i}{q_{i1}^2} + \frac{1-Y_i}{(1-q_{i1})^2} - \left(\frac{\pi_{i1}-\pi_{i0}}{px_i}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{ik1}^2} &= \sum_{i=1}^n \left[\frac{Y_i}{\left(p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}\right)^2} - \left(\frac{pxy_{i1}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{ik0}^2} &= \sum_{i=1}^n \left[\frac{1-Y_i}{\left(p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}\right)^2} - \left(\frac{pxy_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
\end{aligned}
\]
Off-diagonal entries
\[
\begin{aligned}
\frac{\partial^2L}{\partial q_{i0}p_{ik1}} &= \sum_{i=1}^n \frac{\pi_{i1}\pi_{i0}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}px_i^2} \\
\frac{\partial^2L}{\partial q_{i0}p_{ik0}} &= \sum_{i=1}^n \frac{-\pi_{i1}\pi_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}px_i^2} \\
\frac{\partial^2L}{\partial p_{ik1}p_{il1}} &= \sum_{i=1}^n \frac{pxy_{i1}pxy_{i0}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}} p_{il1}^{x_{il}}(p_{il1}-1)^{1-x_{il}} px_i^2}, \text{ where } k \neq l \\
\frac{\partial^2L}{\partial p_{ik0}p_{il0}} &= \sum_{i=1}^n \frac{pxy_{i1}pxy_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}} p_{il0}^{x_{il}}(p_{il0}-1)^{1-x_{il}} px_i^2}, \text{ where } k \neq l \\
\frac{\partial^2L}{\partial p_{ik1}p_{il0}} &= \sum_{i=1}^n \frac{-pxy_{i1}pxy_{i0}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}} p_{il0}^{x_{il}}(p_{il0}-1)^{1-x_{il}} px_i^2}, \forall k, l \in [1,m]
\end{aligned}
\]

## FIM
Since FIM entries are expectations of the second derivatives, we need to take expectations of the diagonal entries that contain $Y_i$,
\[
\begin{aligned}
E\left(\frac{\partial^2L}{\partial q_{i1}^2}\right) &= \sum_{i=1}^n \left[\frac{\pi_{i1}}{q_{i1}px_i} + \frac{\pi_{i0}}{(1-q_{i1})px_i} - \left(\frac{\pi_{i1}-\pi_{i0}}{px_i}\right)^2\right] \\
E\left(\frac{\partial^2L}{\partial p_{ik1}^2}\right) &= \sum_{i=1}^n \left[ \frac{pxy_{i1}}{\left(p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}\right)^2 px_i} - \left(\frac{pxy_{i1}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
E\left(\frac{\partial^2L}{\partial p_{ik0}^2}\right) &= \sum_{i=1}^n \left[\frac{pxy_{i0}}{\left(p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}\right)^2 px_i} - \left(\frac{pxy_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
\end{aligned}
\]

## MML of Naive Bayes
\[
\begin{aligned}
I = -\ln K - \ln h(\vec{\theta}) + \frac{1}{2}\ln F(\vec{\theta}) - \ln f(D|\vec{\theta}) + \frac{|\vec{\theta}|}{2}
\end{aligned}
\]
where $\vec{\theta} = <p_{i0}, p_{ij1}, p_{ij2}>, \forall j \in [1,m]$ is the set of parameters, $|\vec{\theta}|$ is the number of free parameters, $K$ is the lattice contant and $h(\vec{\theta})$ is the parameter prior. A commonly used conjugate prior for binary variables is beta prior (i.e., beta distribution) with probability density function 
\[
f(x, \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}
\]
where $B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$. For simplicity, we assume all parameters are uniformly (i.e., $\alpha=\beta=1$), hence for a single parameter prior is $x(1-x)$. Assuming all parameters are independent, we have  
\[
\ln h(\vec{\theta}) = \ln p_{i0} + \ln (1-p_{i0}) + \sum_{j=1}^m\left[\ln p_{ij1} + \ln (1-p_{ij1}) + \ln p_{ij2} + \ln (1-p_{ij2})\right]
\]
Substituting this into the above MML formula we get 
\[
I = -\left(\ln p_{i0} + \ln (1-p_{i0}) + \sum_{j=1}^m\left[\ln p_{ij1} + \ln (1-p_{ij1}) + \ln p_{ij2} + \ln (1-p_{ij2})\right]\right) + \frac{1}{2}F(\vec{\theta}) - \ln f(D|\vec{\theta}) + \frac{d}{2}(1+\ln k_d)
\]
where $d$ is the number of free parameters and $k_d$ is the lattice constant for each free parameter. 

## Random model
```{r, eval=F}
dag = empty.graph(c("Y", paste0("X", 1:10)))
for (i in 2:9) {
  dag = set.arc(dag, nodes(dag)[1], nodes(dag)[i])
}
#dag = randDag(10, 2)
#graphviz.plot(dag)
cpts = randCPTs(dag, 2, 1)
sampleSize = n = 10000
data = rbn(cpts, n)
vars = colnames(data)
arities = sapply(data, nlevels)
names(arities) = c()
y = "Y"
#x = bnlearn::mb(dag, y)
yIndex = which(vars == y)
#xIndices = which(vars %in% x)
#mb(dag, y)
```

## Executing MML_NB
```{r, eval = T}
for (i in 2:length(vars)) {
  indices = c(i)
  pars = mle_nb(data, vars, indices, yIndex, 0.5)
  probSign = get_prob_sign(data)
  (i_nb = mml_nb(data, probSign, vars, arities, sampleSize, x=vars[indices], y, debug = TRUE))
  #forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb, debug = F)
}

```
## MML_Logit

```{r}
dataNumeric = factor2numeric(data)
vars = names(data)
sampleSize = n
(i_lr = mml_logit(data, arities, sampleSize, vars[indices], y, sigma = 3, debug = T))
#forward_greedy(data, arities, vars, sampleSize, y, score = mml_logit, dataNumeric = dataNumeric, debug = F)
```

## MML_CPT
```{r}
indexListPerNodePerValue = count_occurance(data, arities)
#forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, y, base = exp(1), debug = F)
(i_cpt = mml_cpt(indexListPerNodePerValue, arities, sampleSize, indices, yIndex, base = exp(1)))
dagCPT = empty.graph(c(y, vars[indices]))
for (i in 2:nnodes(dagCPT)) {
  dagCPT = set.arc(dagCPT, nodes(dagCPT)[i], nodes(dagCPT)[1])
}
cptsEst = bn.fit(dagCPT, data[, c(yIndex, indices)], method = "bayes")
ss = 0 
for (i in 1:nrow(data)) {
  valueIndices = as.numeric(data[i, c(yIndex, indices)])
  ss = ss + log(do.call("[", c(list(cptsEst[[1]]$prob), as.list(valueIndices))))
}
cat("@ 1st part:", i_cpt + ss, "\n")
cat("@ 2nd part:", -ss, "\n")
```
The above mml + cpt is even smaller than mml + nb, which is expected to be shorter due to less number of parameters comparing with a full cpt!!!

## Comparison
```{r, eval = F}
i_nb
i_cpt
i_lr
```

## Observations
* In mml_nb, det(fim) are sometimes small negative value, not sure what's the problem. In addition, sometimes det(fim) is very small positive value, hence log(det(fim)) is a large negative value, which makes the first part of mml_nb negative, this is not right!

* singular matrix, columns are not linearly independent




