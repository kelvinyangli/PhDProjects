---
title: "mml87_nb"
author: "Kelvin"
date: "27 June 2017"
output:
  pdf_document: default
  html_document: default
---

```{r global_options, include=F, warning=F, message=F, eval=T}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(bnlearn)
source("~/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R")
```
```{r}
dag = empty.graph(c("X1", "X2", "Y"))
dag = set.arc(dag, "Y", "X1")
dag = set.arc(dag, "Y", "X2")
cpts = randCPTs(dag, 2, 1)
data = rbn(cpts, 1000)
# model = naiveBayes(Y ~ ., data = data)
# model$tables
```

## Log likelihood 
For NB with multinomial variables, there are two types of parameters $P(Y=y)$ and $P(X_j =x | Y=y)$. The likelihood of Naive Bayes is then
\[
\begin{aligned}
P(Y|\vec{X}) &= \frac{P(Y)\prod_{j=1}^m P(X_j|Y)}{P(\vec{X})} \\
&= \frac{P(Y)\prod_{j=1}^m P(X_j|Y)}{\sum_{Y}P(Y)\prod_{j=1}^m P(X_j|Y)}
\end{aligned}
\]
where $\vec{X} = <X_1, \dots, X_m>$ is a vector of $m$ input variables. The negative loglikelihood of an entire data set with $n$ observations is 
\[
\begin{aligned}
L &= -\sum_{i=1}^n \log P(Y|X) \\
&= -\sum_{i=1}^n \left[\log P(Y) + \sum_{j=1}^m \log P(X_j|Y) - \log \left(\sum_{Y}P(Y)\prod_{j=1}^m P(X_j|Y)\right)\right]
\end{aligned}
\]
Since it is not known whether or not there is a closed form solution for the determinant of the FIM, we firstly use MLE of NB parameters
\[
\begin{aligned}
P(Y=y) &= \frac{count(y)}{n} \\
P(X_j = x|Y = y) &= \frac{count(x) \cup count(y)}{count(y)}
\end{aligned}
\]
The following are implementations of MLE of NB parameters. 

## MLE implementations
A function to calculate maximum likelihood estimation of nb parameters. 

```{r}
# y is the output node
# x is a set of input nodes
# estimated parameters are stored in a list
# smoothing is additive smoothing constant to avoid 0 probabilities
# it can take any other values
mle_est_nb = function(y, x, data, smoothing = 1) {
  xIndices = which(colnames(data) %in% x)
  yIndex = which(colnames(data) == y)
  lst = list()
  for (i in 1:length(x)) {
    lst[[i]] = (table(data[, c(yIndex, xIndices[i])]) + smoothing) /
      (rowSums(table(data[, c(yIndex, xIndices[i])])) + smoothing * nlevels(data[, xIndices[i]]))
  } # end for i 
  lst[[yIndex]] = (table(data[, yIndex]) + smoothing) / (nrow(data) + smoothing * nlevels(data[, 3]))
  names(lst) = c(x, y)
  return(lst)
}

# (mle_est_nb("Y", c("X1", "X2"), data, 0)) # with no smoothing
(pars = mle_est_nb("Y", c("X1", "X2"), data, 1)) # with smoothing
```

## Fisher information matrix

Define $\theta_0 = P(Y=y)$ and $\theta_j = P(X_j = x|Y=y), \forall j \in [1,m]$. The second derivative of the above negative log likelihood w.r.t each parameter is 
\[
\begin{aligned}
\frac{\partial^2L}{\partial\theta_k^2} &= \sum_{i=1}^n \left[\frac{1}{\theta_k^2} - \left(\frac{\prod_{j=0}^{\neq k}\theta_j}{\sum_y\prod_{j=0}^m\theta_j}\right)^2\right] \\
\frac{\partial^2L}{\partial\theta_k\partial\theta_l} &= \sum_{i=1}^n \left[\frac{(\sum_y\prod_{j=0}^m\theta_j)(\sum_{j=0}^{\neq k, l}\theta_j)-(\prod_{j=0}^{\neq k}\theta_j)(\prod_{j=0}^{\neq l}\theta_j)}{\left(\sum_y\prod_{j=0}^m\theta_j\right)^2}\right]
\end{aligned}
\]

<!-- to be a vector of parameters for Naive Bayes, where $\theta_0 = P(Y=y_1)$ and -->
<!-- $\theta{j1} = P(X_j = x_j | Y = y_1)$ and $\theta{j2} = P(X_j = x_j | Y = y_2)$ -->
<!-- respectively. Then we have the following -->
<!-- \[ -->
<!-- \begin{aligned} -->
<!-- P(X) &= \sum_{Y=y} P(Y=y) \prod_{j=1}^m P(X_j|Y=y) \\ -->
<!-- \frac{\partial^2L}{\partial\theta_0^2} &= \sum_{i=1}^n \left[\frac{1}{\theta_0^2} - \left(\frac{\prod_{j=1}^m\theta_{j1} - \prod_{j=1}^m\theta_{j2}}{P(X)}\right)^2\right] \\ -->
<!-- \frac{\partial^2L}{\partial\theta_{k1}^2} &= \sum_{i=1}^n \left[\frac{1}{\theta_{k1}^2} - \left(\frac{\theta_0\prod_{j=1}^{m,j \neq k}\theta_{j1}}{P(X)}\right)^2\right] \\ -->
<!-- \frac{\partial^2L}{\partial\theta_{k2}^2} &= \sum_{i=1}^n \left[-\left(\frac{(1-\theta_0)\prod_{j=1}^{m,j \neq k}\theta_{j2}}{P(X)}\right)^2\right] -->
<!-- \end{aligned} -->
<!-- \] -->
<!-- for $k \in [1, m]$. -->

## FIM implementations
```{r}
y = "Y"
x = c("X1", "X2")
arities = sapply(data, nlevels)
yIndex = which(colnames(data) == y)
xIndices = which(colnames(data) %in% x)

px = function(yIndex, xIndices, pars, dataPoint, arities) {
  ss = 0 
  for (yValue in 1:arities[yIndex]) {
    mm = pars[[yIndex]][yValue]
    for (j in 1:length(xIndices)) {
      #xValue = data[dataPoint, xIndices[j]]
      xValue = dataPoint[[xIndices[j]]]
      mm = mm * pars[[j]][yValue, xValue]
    } # end for each x_j
    ss = ss + mm
  } # end for each y value
  return(ss)
}

#head(data)
#pars
px(3, c(1,2), pars, data[101,], arities)
head(apply(data, 1, px, yIndex=3,xIndices=c(1,2),pars=pars,arities=arities))
```






