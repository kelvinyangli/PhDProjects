---
title: "mml87_nb"
author: "Kelvin"
date: "27 June 2017"
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=F, warning=F, message=F, eval=T, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = F, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))

logFactorialSheet = read.csv(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/logFactorial_1to10000.csv")) 
```

## MLE of NB parameters
The MLE estimations of NB parameters are as follows:
\[
\begin{aligned}
P(Y=y) &= \frac{count(y)}{n} \\
P(X_j = x|Y = y) &= \frac{count(x) \cup count(y)}{count(y)}
\end{aligned}
\]

## Negative log likelihood of NB
For a Naive Bayes model with binary variable, its parameters are $\{P(Y_i=T), P(X_{ij}|Y_i=T), P(X_{ij}|Y_i=F)\}$. To simply the notations, we use $\{p_{i0}, p_{ij1}, p_{ij2}\}$ to denote the above probabilities respectively. The likelihood of Naive Bayes given a data set is then
\[
\begin{aligned}
l = \prod_i^n P(Y_i=T|\vec{X_i})^{Y_i} (1-P(Y_i=T|\vec{X_i}))^{1-Y_i}
\end{aligned}
\]
where the posterior probability of $Y_i$ given a vector $\vec{X_i} = <X_{i1}, \dots, X_{im}>$ is 
\[
\begin{aligned}
P(Y_i=T|\vec{X_i}) &= \frac{P(Y_i=T)\prod_{j=1}^m P(X_{ij}|Y_i=T)}{P(\vec{X_i})} \\
&= \frac{P(Y_i=T)\prod_{j=1}^m P(X_{ij}|Y_i=T)}{P(Y_i=T)\prod_{j=1}^m P(X_{ij}|Y_i=T)+(1-P(Y_i=T))\prod_{j=1}^m P(X_{ij}|Y_i=F)} \\
&= \frac{p_{i0}\prod_{j=1}^m p_{ij1}}{p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}}
\end{aligned}
\]
The negative loglikelihood
\[
\begin{aligned}
L &= -\sum_{i=1}^n \left[Y_i\ln p(Y_i|\vec{X_i}) + (1-Y_i) \ln (1-p(Y_i|\vec{X_i}))\right] \\
&= -\sum_{i=1}^n \left[Y_i \ln p_{i0} + (1-Y_i) \ln (1-p_{i0}) + Y_i \sum_{j=1}^m \ln p_{ij1} + (1-Y_i) \sum_{j=1}^m \ln p_{ij2} - \ln \left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)\right]
\end{aligned}
\]

## Fisher information matrix
The first derivatives of the above negative log likelihood w.r.t. each parameter are
\[
\begin{aligned}
\frac{\partial L}{\partial p_{i0}} &= -\sum_{i=1}^n \left[\frac{Y_i}{p_{i0}} - \frac{1-Y_i}{1-p_{i0}} -\frac{\prod_{j=1}^m p_{ij1} - \prod_{j=1}^m p_{ij2}}{p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}} \right] \\
\frac{\partial L}{\partial p_{ik1}} &= -\sum_{i=1}^n \left[\frac{Y_i}{p_{ik1}} - \frac{p_{i0}\prod_{j=1}^m p_{ik1}}{p_{ik1}\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)}\right] \\
\frac{\partial L}{\partial p_{ik2}} &= -\sum_{i=1}^n \left[\frac{1-Y_i}{p_{ik2}} - \frac{(1-p_{i0})\prod_{j=1}^m p_{ik2}}{p_{ik2}\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)}\right]
\end{aligned}
\]
The second derivatives are
\[
\begin{aligned}
\frac{\partial^2L}{\partial p_{i0}^2} &= \sum_{i=1}^n \left[\frac{Y_i}{p_{i0}^2} + \frac{1-Y_i}{(1-p_{i0})^2} - \left(\frac{\prod_{j=1}^m p_{ij1} - \prod_{j=1}^m p_{ij2}}{p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{ik1}} &= \sum_{i=1}^n \left[\frac{Y_i}{p_{ik1}^2} - \left(\frac{p_{i0} \prod_{j=1}^mp_{ij1}}{p_{ik1}\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{ik2}} &= \sum_{i=1}^n \left[\frac{1-Y_i}{p_{ik2}^2} - \left(\frac{(1-p_{i0}) \prod_{j=1}^mp_{ij2}}{p_{ik2}\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{i0}p_{ik1}} &= \sum_{i=1}^n \frac{\prod_{j=1}^mp_{ij1}p_{ij2}}{\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)^2}\frac{1}{p_{ik1}}\\
\frac{\partial^2L}{\partial p_{i0}p_{ik2}} &= \sum_{i=1}^n \frac{\prod_{j=1}^mp_{ij1}p_{ij2}}{\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)^2}\frac{-1}{p_{ik2}}\\
\frac{\partial^2L}{\partial p_{ik1}p_{ik2}} &= \sum_{i=1}^n \frac{\prod_{j=1}^mp_{ij1}p_{ij2}}{\left(p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}\right)^2}\frac{-p_{i0}(1-p_{i0})}{p_{ik1}p_{ik2}}
\end{aligned}
\]

Since FIM entries are expectations of the second derivatives, we need to take expectations for the first three second derivatives that contain $Y_i$. To simplify the notations, we use $p_x$ to denote $p_{i0}\prod_{j=1}^m p_{ij1} + (1-p_{i0})\prod_{j=1}^m p_{ij2}$. Then the expectations become
\[
\begin{aligned}
E\left(\frac{\partial^2L}{\partial p_{i0}^2}\right) &= \sum_{i=1}^n \left[\frac{\prod_{j=1}^m p_{ij1}}{p_{i0}p_x} + \frac{\prod_{j=1}^m p_{ij2}}{(1-p_{i0})p_x}- \left(\frac{\prod_{j=1}^m p_{ij1} - \prod_{j=1}^m p_{ij2}}{p_x}\right)^2\right] \\
E\left(\frac{\partial^2L}{\partial p_{ik1}}\right) &= \sum_{i=1}^n \left[\frac{p_{i0}\prod_{j=1}^m p_{ij1}}{p_{ik1}^2p_x} - \left(\frac{p_{i0} \prod_{j=1}^mp_{ij1}}{p_{ik1}p_x}\right)^2\right] \\
E\left(\frac{\partial^2L}{\partial p_{ik2}}\right) &= \sum_{i=1}^n \left[\frac{(1-p_{i0})\prod_{j=1}^m p_{ij2}}{p_{ik2}^2p_x} - \left(\frac{(1-p_{i0}) \prod_{j=1}^mp_{ij2}}{p_{ik2}p_x}\right)^2\right] \\
\end{aligned}
\]

## MML of Naive Bayes
\[
\begin{aligned}
I = -\ln K - \ln h(\vec{\theta}) + \frac{1}{2}\ln F(\vec{\theta}) - \ln f(D|\vec{\theta}) + \frac{|\vec{\theta}|}{2}
\end{aligned}
\]
where $\vec{\theta} = <p_{i0}, p_{ij1}, p_{ij2}>, \forall j \in [1,m]$ is the set of parameters, $|\vec{\theta}|$ is the number of free parameters, $K$ is the lattice contant and $h(\vec{\theta})$ is the parameter prior. A commonly used conjugate prior for binary variables is beta prior (i.e., beta distribution) with probability density function 
\[
f(x, \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}
\]
where $B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$. For simplicity, we assume all parameters are uniformly (i.e., $\alpha=\beta=1$), hence for a single parameter prior is $x(1-x)$. Assuming all parameters are independent, we have  
\[
\ln h(\vec{\theta}) = \ln p_{i0} + \ln (1-p_{i0}) + \sum_{j=1}^m\left[\ln p_{ij1} + \ln (1-p_{ij1}) + \ln p_{ij2} + \ln (1-p_{ij2})\right]
\]
Substituting this into the above MML formula we get 
\[
I = -\left(\ln p_{i0} + \ln (1-p_{i0}) + \sum_{j=1}^m\left[\ln p_{ij1} + \ln (1-p_{ij1}) + \ln p_{ij2} + \ln (1-p_{ij2})\right]\right) + \frac{1}{2}F(\vec{\theta}) - \ln f(D|\vec{\theta}) + \frac{d}{2}(1+\ln k_d)
\]
where $d$ is the number of free parameters and $k_d$ is the lattice constant for each free parameter. 

## Random model
```{r}
dag = empty.graph(c("Y", paste0("X", 1:10)))
for (i in 2:nnodes(dag)) {
  dag = set.arc(dag, nodes(dag)[1], nodes(dag)[i])
}
#dag = randDag(20, 4)
graphviz.plot(dag)
cpts = randCPTs(dag, 2, 1)
sampleSize = n = 1000
data = rbn(cpts, n)
vars = colnames(data)
arities = sapply(data, nlevels)
names(arities) = c()
y = "Y"
#x = bnlearn::mb(dag, y)
yIndex = which(vars == y)
#xIndices = which(vars %in% x)
#mb(dag, y)
```

## Executing MML_NB
```{r, eval = T}
indices = c(2,3,4,5,6,7,8,9,10,11)
(i_nb = mml_nb(data, vars, arities, sampleSize, x=vars[indices], y, debug = TRUE))
#forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb, debug = F)
```

## MML_CPT
```{r}
indexListPerNodePerValue = count_occurance(data, arities)
#forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, y, logFactorialSheet, base = exp(1), debug = F)
(i_cpt = mml_cpt(indexListPerNodePerValue, arities, sampleSize, indices, yIndex, logFactorialSheet, base = exp(1)))
```
The above mml + cpt is even smaller than mml + nb, which is expected to be shorter due to less number of parameters comparing with a full cpt!!!

## MML_Logit

```{r}
dataNumeric = factor2numeric(data)
vars = names(data)
sampleSize = n
(i_lr = mml_logit(data, arities, sampleSize, vars[indices], y, sigma = 3, debug = T))
#forward_greedy(data, arities, vars, sampleSize, y, score = mml_logit, dataNumeric = dataNumeric, debug = F)
```

## Sanity check
This is a sanity check to ensure that nll is corrected calculated. The following code use gRain to compute the posterior probability $P(Y_i|\vec{X_i})$ based on the estimated cpts from data. The posterior probability given each data point is then used to compute the nll of the entire data set. The answer confirms that the above nll calculation is correct. 
```{r, eval=F}
cptsEst = bn.fit(dag, data, method = "bayes")
obj = gRbase::compile(as.grain(cptsEst))
post = querygrain(obj, nodes = names(data)[c(yIndex, xIndices)], type = "conditional")
nll2 = 0
for (i in 1:nrow(data)) {
  for (j in 1:length(x)) {
    paste0("x", j) = data[i, xIndices[j]]
  }
  y = data[i, "Y"]
  nll2 = nll2 + log(post[x1, x2, x3, y])
}
-nll2
```

## Comparison
```{r, eval = T}
i_nb
i_cpt
i_lr
```

## Observations

* The above tests show that nll for both naive bayes and logit are close to nll using true cpts. But mml_nb is larger than mml_logit and mml_cpt even if the true model is a naive bayes model. This is likely to be  caused by large message length for nb parameter priors and definitely large log(det(fim)) as compared with mml_logit. Could this because of the mml_logit is only for 1st order logit model, hence the model is simpler than a nb? 

* In mml_nb, det(fim) are sometimes negative when X is a single variable. When Xs are two or more variables, it is less likely to have negative determinant. Not sure what's the problem. This problem may have been fixed. But occasionally the determinant is still a small negative number, perhaps due to under flow. 

* As suggested by Lloyd, did a sanity check by approximating 2nd derivatives using numerical methods. The
sanity check was conducted on a simple example and the results are agreed with analytical solutions. 

* mml smoothing 0.5 * number of states

* mml informative increment per parameter value 

* lattice constant 1/2*pi*e

* first part of nb, double check 0.5 constant of log fisher

