---
title: "mml87_nb"
author: "Kelvin"
date: "27 June 2017"
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=F, warning=F, message=F, eval=F, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = T, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))
```

## Random model
```{r, eval=T}
nXs = 10
dag = empty.graph(c("Y", paste0("X", 1:nXs)))
for (i in 2:nnodes(dag)) {
  dag = set.arc(dag, nodes(dag)[1], nodes(dag)[i])
}
#dag = randDag(10, 2)
#graphviz.plot(dag)
cpts = randCPTs(dag, 2, 1)
sampleSize = n = 1000
data = rbn(cpts, n)
vars = colnames(data)
arities = sapply(data, nlevels)
names(arities) = c()
y = "Y"
#x = bnlearn::mb(dag, y)
yIndex = which(vars == y)
#xIndices = which(vars %in% x)
#mb(dag, y)
#cpts
```

## Executing MML_NB
```{r, eval = T}
#for (i in 2:length(vars)) {
  i = 11
  indices = c(i)
  I = mml_nb_adaptive(data, yIndex, indices)
  pars = mle_nb(data, vars, indices, yIndex, 0.5)
  nll = -sum(apply(data, 1, nll_nb, pars = pars, xIndices = indices, yIndex = yIndex))
  # p(y=T)
  py1 = pars[[length(pars)]][[1]]
  # p(y=F)
  py0 = pars[[length(pars)]][[2]]
  if (length(indices) > 0) {# if x is not empty
    # a vector of \prod_j p(x_ij|y=1st value) = \prod_j p(x_ij|y=1)
    prodPij1 = apply(data, 1, prod_pijk, pars = pars, xIndices = indices, yValue = 1)
    # a vector of \prod_j p(x_ij|y=2nd value) = \prod_j p(x_ij|y=0)
    prodPij0 = apply(data, 1, prod_pijk, pars = pars, xIndices = indices, yValue = 2)
    # a vector of p_xi 
    px = py1 * prodPij1 + py0 * prodPij0
    nll = nll + sum(log(px)) # add additional log(px) value to nll when x is not empty
  }
  # probSign = get_prob_sign(data)
  # (i_nb = mml_nb(data, probSign, vars, arities, sampleSize, x=vars[indices], y, debug = TRUE))
  #forward_greedy(data, arities, vars, sampleSize, y, score = mml_nb, probSign = probSign, debug = T)
#}
  cat("mml_nb=", I, "\n")
  cat("@1st part=", I-nll, "\n")
  cat("@2nd part=", nll, "\n")
  

## MML_Logit


dataNumeric = factor2numeric(data)
vars = names(data)
sampleSize = n
i_lr = mml_logit(data, arities, sampleSize, vars[indices], y, sigma = 3, debug = T)
#forward_greedy(data, arities, vars, sampleSize, y, score = mml_logit, dataNumeric = dataNumeric, debug = F)


## MML_CPT

indexListPerNodePerValue = count_occurance(data, arities)
#forward_greedy_fast(data, indexListPerNodePerValue, arities, sampleSize, y, base = exp(1), debug = F)
(i_cpt = mml_cpt(indexListPerNodePerValue, arities, sampleSize, indices, yIndex, base = exp(1)))
dagCPT = empty.graph(c(y, vars[indices]))
for (i in 2:nnodes(dagCPT)) {
  dagCPT = set.arc(dagCPT, nodes(dagCPT)[i], nodes(dagCPT)[1])
}
cptsEst = bn.fit(dagCPT, data[, c(yIndex, indices)], method = "bayes")
ss = 0 
for (i in 1:nrow(data)) {
  valueIndices = as.numeric(data[i, c(yIndex, indices)])
  ss = ss + log(do.call("[", c(list(cptsEst[[1]]$prob), as.list(valueIndices))))
}
cat("@ 1st part:", i_cpt + ss, "\n")
cat("@ 2nd part:", -ss, "\n")
```
The above mml + cpt is even smaller than mml + nb, which is expected to be shorter due to less number of parameters comparing with a full cpt!!!

## Comparison
```{r, eval = F}
i_nb
i_cpt
i_lr
```

## Observations
* In mml_nb, det(fim) are sometimes small negative value, not sure what's the problem. In addition, sometimes det(fim) is very small positive value, hence log(det(fim)) is a large negative value, which makes the first part of mml_nb negative, this is not right!

* singular matrix, columns are not linearly independent

* try: change values of parameters to see how sensitive the log likelihood is 

* try: plot the log likelihood to see its shape 

* mml_nb_adaptive works well, comparing with mml_cpt, we don't add the extra bit for each parameter, because we don't know what's the difference b/w nb_adaptive and nb_87, but maybe we shouldn't add the extra bit for mml_cpt either because all we want to know is the message length of a node given its parents, we are not interested in the mml estimation of parameters 

* strange: after using a different log factorial approximation in mml_cpt, the results are worse than before

* mml_nb_adaptive: the code works, and seems working ok, but the results using mml_nb alone comparing with mml_cpt are flucturating, think about this after mid-review.







