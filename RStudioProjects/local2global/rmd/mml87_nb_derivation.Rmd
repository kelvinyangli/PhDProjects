---
title: "mml87_nb_derivation"
author: "kl"
date: "25 August 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## MLE of NB parameters
The MLE estimations of NB parameters are as follows:
\[
\begin{aligned}
P(Y=y) &= \frac{count(y)}{n} \\
P(X_j = x|Y = y) &= \frac{count(x) \cup count(y)}{count(y)}
\end{aligned}
\]

## Negative log likelihood of NB
For a Naive Bayes model with binary variable, its parameters are $\{P(Y_i=1), P(X_{ij}=1|Y_i=1), P(X_{ij}=1|Y_i=0)\}$. To simply the notations, we use $\{q_{i1}, p_{ij1}, p_{ij0}\}$ to denote the above probabilities respectively, and denote 
\[
\begin{aligned}
\pi_{i1} &= \prod_{j=1}^m p_{ij1}^{x_i}(1-p_{ij1})^{1-x_i} \\
\pi_{i0} &= \prod_{j=1}^m p_{ij0}^{x_i}(1-p_{ij0})^{1-x_i} \\
pxy_{i1} &= q_{i1}\pi_{i1} \\
pxy_{i0} &= (1-q_{i1})\pi_{i0} \\
p(\vec{X_i}) &= px_i = pxy_{i1} + pxy_{i0}
\end{aligned}
\]
The likelihood of Naive Bayes given a data set is then
\[
\begin{aligned}
l = \prod_i^n P(Y_i=1|\vec{X_i})^{Y_i} (1-P(Y_i=1|\vec{X_i}))^{1-Y_i}
\end{aligned}
\]
where the posterior probability of $Y_i$ given a vector $\vec{X_i} = <X_{i1}, \dots, X_{im}>$ is 
\[
\begin{aligned}
P(Y_i=T|\vec{X_i}) &= \frac{P(Y_i=1)\prod_{j=1}^m P(X_{ij}=1|Y_i=1)^{X_i}(1-P(X_{ij}=1|Y_i=1))^{1-X_i}}{P(\vec{X_i})} \\
&= \frac{q_{i1}\pi_{i1}}{px_i}
\end{aligned}
\]
The negative loglikelihood
\[
\begin{aligned}
L &= -\sum_{i=1}^n \left[Y_i\ln p(Y_i=1|\vec{X_i}) + (1-Y_i) \ln (1-p(Y_i=1|\vec{X_i}))\right] \\
&= -\sum_{i=1}^n \left[Y_i \ln q_{i1} + (1-Y_i) \ln (1-q_{i1}) + Y_i \ln \pi_{i1} + (1-y_i) \ln \pi_{i0} - \ln px_i\right]
\end{aligned}
\]

## First derivatives
Before differentiating nll w.r.t. each parameter, we derive the followings first
\[
\begin{aligned}
\frac{\partial}{\partial q_{i1}}px_i &= \pi_{i1} - \pi_{i0} \\
\frac{\partial}{\partial p_{ij1}}\pi_{i1} &= \frac{\pi_{i1}}{p_{ij1}^{x_{ij}}(p_{ij1}-1)^{1-x_{ij}}} \\
\frac{\partial}{\partial p_{ij0}}\pi_{i0} &= \frac{\pi_{i0}}{p_{ij0}^{x_{ij}}(p_{ij0}-1)^{1-x_{ij}}} \\
\frac{\partial}{\partial p_{ij1}}px_i &= \frac{\partial}{\partial p_{ij1}}pxy_{i1} = \frac{pxy_{i1}}{p_{ij1}^{x_{ij}}(p_{ij1}-1)^{1-x_{ij}}} \\
\frac{\partial}{\partial p_{ij0}}px_i &= \frac{\partial}{\partial p_{ij0}}pxy_{i0} = \frac{pxy_{i0}}{p_{ij0}^{x_{ij}}(p_{ij0}-1)^{1-x_{ij}}}
\end{aligned}
\]
The first derivatives of the negative log likelihood w.r.t. each parameter are
\[
\begin{aligned}
\frac{\partial L}{\partial q_{i1}} &= -\sum_{i=1}^n \left[\frac{Y_i}{q_{i1}} - \frac{1-Y_i}{1-q_{i1}} -\frac{\pi_{i1}-\pi_{i0}}{px_i} \right] \\
\frac{\partial L}{\partial p_{ik1}} &= -\sum_{i=1}^n \left[\frac{Y_i}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}} - \frac{pxy_{i1}}{p_{ik1}^{x_{ij}}(p_{ik1}-1)^{1-x_{ij}}px_i}\right] \\
\frac{\partial L}{\partial p_{ik0}} &= -\sum_{i=1}^n \left[\frac{1-Y_i}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}} - \frac{pxy_{i0}}{p_{ik0}^{x_{ij}}(p_{ik0}-1)^{1-x_{ij}}px_i}\right] \\
\end{aligned}
\]

## Second derivatives
Diagonal entries 
\[
\begin{aligned}
\frac{\partial^2L}{\partial q_{i1}^2} &= \sum_{i=1}^n \left[\frac{Y_i}{q_{i1}^2} + \frac{1-Y_i}{(1-q_{i1})^2} - \left(\frac{\pi_{i1}-\pi_{i0}}{px_i}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{ik1}^2} &= \sum_{i=1}^n \left[\frac{Y_i}{\left(p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}\right)^2} - \left(\frac{pxy_{i1}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
\frac{\partial^2L}{\partial p_{ik0}^2} &= \sum_{i=1}^n \left[\frac{1-Y_i}{\left(p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}\right)^2} - \left(\frac{pxy_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
\end{aligned}
\]
Off-diagonal entries
\[
\begin{aligned}
\frac{\partial^2L}{\partial q_{i0}p_{ik1}} &= \sum_{i=1}^n \frac{\pi_{i1}\pi_{i0}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}px_i^2} \\
\frac{\partial^2L}{\partial q_{i0}p_{ik0}} &= \sum_{i=1}^n \frac{-\pi_{i1}\pi_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}px_i^2} \\
\frac{\partial^2L}{\partial p_{ik1}p_{il1}} &= \sum_{i=1}^n \frac{pxy_{i1}pxy_{i0}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}} p_{il1}^{x_{il}}(p_{il1}-1)^{1-x_{il}} px_i^2}, \text{ where } k \neq l \\
\frac{\partial^2L}{\partial p_{ik0}p_{il0}} &= \sum_{i=1}^n \frac{pxy_{i1}pxy_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}} p_{il0}^{x_{il}}(p_{il0}-1)^{1-x_{il}} px_i^2}, \text{ where } k \neq l \\
\frac{\partial^2L}{\partial p_{ik1}p_{il0}} &= \sum_{i=1}^n \frac{-pxy_{i1}pxy_{i0}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}} p_{il0}^{x_{il}}(p_{il0}-1)^{1-x_{il}} px_i^2}, \forall k, l \in [1,m]
\end{aligned}
\]

## FIM
Since FIM entries are expectations of the second derivatives, we need to take expectations of the diagonal entries that contain $Y_i$,
\[
\begin{aligned}
E\left(\frac{\partial^2L}{\partial q_{i1}^2}\right) &= \sum_{i=1}^n \left[\frac{\pi_{i1}}{q_{i1}px_i} + \frac{\pi_{i0}}{(1-q_{i1})px_i} - \left(\frac{\pi_{i1}-\pi_{i0}}{px_i}\right)^2\right] \\
E\left(\frac{\partial^2L}{\partial p_{ik1}^2}\right) &= \sum_{i=1}^n \left[ \frac{pxy_{i1}}{\left(p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}\right)^2 px_i} - \left(\frac{pxy_{i1}}{p_{ik1}^{x_{ik}}(p_{ik1}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
E\left(\frac{\partial^2L}{\partial p_{ik0}^2}\right) &= \sum_{i=1}^n \left[\frac{pxy_{i0}}{\left(p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}\right)^2 px_i} - \left(\frac{pxy_{i0}}{p_{ik0}^{x_{ik}}(p_{ik0}-1)^{1-x_{ik}}px_i}\right)^2\right] \\
\end{aligned}
\]

## MML of Naive Bayes
\[
\begin{aligned}
I = -\ln K - \ln h(\vec{\theta}) + \frac{1}{2}\ln F(\vec{\theta}) - \ln f(D|\vec{\theta}) + \frac{|\vec{\theta}|}{2}
\end{aligned}
\]
where $\vec{\theta} = <p_{i0}, p_{ij1}, p_{ij2}>, \forall j \in [1,m]$ is the set of parameters, $|\vec{\theta}|$ is the number of free parameters, $K$ is the lattice contant and $h(\vec{\theta})$ is the parameter prior. A commonly used conjugate prior for binary variables is beta prior (i.e., beta distribution) with probability density function 
\[
f(x, \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}
\]
where $B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$. For simplicity, we assume all parameters are uniformly (i.e., $\alpha=\beta=1$), hence for a single parameter prior is $x(1-x)$. Assuming all parameters are independent, we have  
\[
\ln h(\vec{\theta}) = \ln p_{i0} + \ln (1-p_{i0}) + \sum_{j=1}^m\left[\ln p_{ij1} + \ln (1-p_{ij1}) + \ln p_{ij2} + \ln (1-p_{ij2})\right]
\]
Substituting this into the above MML formula we get 
\[
I = -\left(\ln p_{i0} + \ln (1-p_{i0}) + \sum_{j=1}^m\left[\ln p_{ij1} + \ln (1-p_{ij1}) + \ln p_{ij2} + \ln (1-p_{ij2})\right]\right) + \frac{1}{2}F(\vec{\theta}) - \ln f(D|\vec{\theta}) + \frac{d}{2}(1+\ln k_d)
\]
where $d$ is the number of free parameters and $k_d$ is the lattice constant for each free parameter. 

