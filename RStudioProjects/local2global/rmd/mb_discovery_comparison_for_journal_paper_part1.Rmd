---
title: "mb_discovery_comparison_for_journal_paper_part1"
author: "kl"
date: "31 January 2018"
output: html_document
---

```{r global_options, include=F, warning=F, eval=F, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = F, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))

```

These exp take 5 parameters into account, ordered by priority: 

* nvars 10, 30, 50, 100
* n 50, 500, 5000
* npa 2, 4, 6
* narity 2, 3, 4
* beta (derichlet concentration parameter) 1, 5, 25

```{r}
dir = "~/Documents/Experiments/kdd_exp/"
vnvars = c(10, 50, 100)
vnpa = c(2, 4, 6)
vnarity = c(2, 3, 4)
vbeta = c(1, 5, 25)
vn = c(50, 500, 5000)
for (nvars in vnvars) {
  
  for (npa in vnpa) {
          
    for (narity in vnarity) {
      
      for (beta in vbeta) {
        nvars = 50
        npa = narity = 2 
        beta = 1
        dirname = paste(nvars, npa, narity, beta, sep = "_")
        
        for (itr in 1:5) {
  
          sd = randSeed()
          set.seed(sd)
          dag = randDag(nvars, npa)
          cpts = randCPTs(dag, narity, beta)
          nodes = bnlearn::nodes(dag)
          mbtrue = list()
          i = 1
          for (x in nodes) {
            mbtrue[[i]] = bnlearn::mb(dag, x)
            i = i + 1
          }
          saveRDS(dag, paste0(dir, dirname, "/dag/dag_", sd, ".rds"))
          saveRDS(cpts, paste0(dir, dirname, "/cpts/cpt_", sd, ".rds"))
          saveRDS(mbtrue, paste0(dir, dirname, "/mb_true/mb_", sd, ".rds"))
          
          for (n in vn) {
            
            datasd = randSeed()
            set.seed(datasd)
            data = rbn(cpts, n)  
            saveRDS(data, paste0(dir, dirname, "/data_rds/data_", n, "_", sd, "_", datasd, ".rds"))
            write.csv(data, paste0(dir, dirname, "/data_csv/data_", n, "_", sd, "_", datasd, ".csv"), row.names = F)
            
          }
        }# end itr
      }# end beta
    }
  }
}

```


## mml + cpt, nb, rand learning using parallel computing on real models 
```{r}
model = "child"
dir = "~/Documents/Experiments/kdd_exp/"
n = sampleSize = 500
datasets = list.files(paste0(dir, model, "/data_csv/", n))

### starting parallel computing ###
no_cores = 4
registerDoParallel(no_cores)
for (ii in 1:length(datasets)) {
  filename = strsplit(datasets[ii], ".csv")[[1]]
  mbcpt = mbnb = mbrandv = list()
  data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii])) 
  vars = colnames(data)
  nvars = length(vars)
  data_cat = numeric2categorical(data)
  arities = sapply(data_cat, nlevels)
  di = varCnt = count_occurance(data_cat, arities)
  data_num = data.matrix(data_cat)

  # mbcpt = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar% {
  #   forward_greedy_fast(data, di, arities, vars, n, target)
  #     }
  # saveRDS(mbcpt, paste0(dir, model, "/mml_cpt_est_prior/", filename, ".rds"))

  # mbnb = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "nb")
  # saveRDS(mbnb, paste0(dir, model, "/mml_nb/", filename, ".rds"))
  # 
  mbrand = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar%
    forward_greedy(data_num, arities, vars, n, target, "random", varCnt = di, prior = "uniform")
  saveRDS(mbrand, paste0(dir, model, "/mml_rand/", filename, ".rds"))

}

stopImplicitCluster()
```


## mml + cpt, nb, rand learning using parallel computing on artificial models
```{r}
# library(parallel)
# library(foreach)
# library(doParallel)
dir = "~/Documents/Experiments/kdd_exp/"
model = "50_5_4_1"
n = 5000
datasets = list.files(paste0(dir, model, "/data_rds/", n))
#datasets = list.files(paste0(dir, model, "/data_rds/", n))
#dags = list.files(paste0(dir, params, "/dag/"))

## starting parallel computing ###
no_cores = 3
registerDoParallel(no_cores)
for (ii in 1:length(datasets)) {
  #cat(i, "\n")
  #data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
  data = readRDS(paste0(dir, model, "/data_rds/", n, "/", datasets[ii]))
  filename = strsplit(datasets[ii], ".rds")[[1]]
  # nvars = ncol(data)
  vars = names(data)
  # #data_cat = numeric2categorical(data)
  # arities = sapply(data, nlevels)
  # varCnt = count_occurance(data, arities)
  # data = data.matrix(data)
  
  mbiamb = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar% {
    learn.mb(x = data, node = target, method = "iamb", alpha = 0.05)
      }
  saveRDS(mbiamb, paste0(dir, model, "/iamb_bnlearn/iamb_", filename))
  
  # for (target in vars) {
  #   targetIndex = which(vars == target)
  #   alpha = alpha_est[[which(sapply(alpha_est, length) == arities[targetIndex])]]
  #   forward_greedy_fast(data, varCnt, arities, vars, n, target, alpha, debug = F)
  # }
  
  # mbcpt = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar% {
  #   forward_greedy_fast(data, varCnt, arities, vars, n, target, alpha=1)
  #     }
  # saveRDS(mbcpt, paste0(dir, model, "/mml_cpt/cpt_", filename))
  
  # mbnb = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "nb")
  # saveRDS(mbnb, paste0(dir, model, "/mml_nb/nb_", filename))
  # 
  # mbrand = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "random", varCnt = varCnt, prior = "uniform")
  # saveRDS(mbrand, paste0(dir, model, "/mml_rand/rand_", filename))
  
}

stopImplicitCluster()
```


## pcmb and iamb in cpp on artificial models
consider alpha = 0.05 for pcmb and iamb (because stick to the original paper's exp setting in tsamardinos and sll)
```{r}
setwd("~/Documents/Experiments/kdd_exp/PCMB/")
dir = "~/Documents/Experiments/kdd_exp/"
for (model in c("30_5_4_1")) {
  for (n in c(10000)) {
    datasets = list.files(paste0(dir, "/", model, "/data_csv/", n))
    #file.remove("output.txt")
    for (ii in 1:length(datasets)) {
      
      data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
      vars = colnames(data)
      nvars = ncol(data)
      # copying data into pcmb directory, overwrite the previous data
      file.copy(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]), paste0(model, ".data"), overwrite = TRUE) 
      res_pcmb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 1 0.05"), intern = TRUE)
      res_iamb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 0 0.05"), intern = TRUE)
      mbl_pcmb = parsePCMB(res_pcmb, nvars, vars)
      mbl_iamb = parsePCMB(res_iamb, nvars, vars)
      filename = strsplit(datasets[ii], ".csv")[[1]]
      
      saveRDS(mbl_pcmb, paste0("../", model, "/pcmb/", filename, ".rds")) # save learned mb as .rds
      saveRDS(mbl_iamb, paste0("../", model, "/iamb_cpp/", filename, ".rds")) # save learned mb as .rds
      #file.remove("output.txt")
      
    }
  }
}

setwd("~/Documents/PhDProjects/RStudioProjects/lglbnlearn/")
```


## pcmb and iamb in cpp on real models
consider alpha = 0.05 for pcmb and iamb (because stick to the original paper's exp setting in tsamardinos and sll)
```{r, }
setwd("~/Documents/Experiments/kdd_exp/PCMB/")
dir = "~/Documents/Experiments/kdd_exp/"
for (model in c("barley", "hailfinder")) {
  for (n in c(500, 1000, 5000)) {
    datasets = list.files(paste0(dir, "/", model, "/data_csv/", n))
    #file.remove("output.txt")
    for (ii in 1:length(datasets)) {
      
      data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
      
      vars = colnames(data)
      nvars = ncol(data)
      # copying data into pcmb directory, overwrite the previous data
      file.copy(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]), paste0("~/Documents/Experiments/kdd_exp/PCMB/", model, ".data"), overwrite = TRUE) 
      res_pcmb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 1 0.05"), intern = TRUE)
      res_iamb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 0 0.05"), intern = TRUE)
      mbl_pcmb = parsePCMB(res_pcmb, nvars, vars)
      #cat(unlist(mbl_pcmb), "\n")
      mbl_iamb = parsePCMB(res_iamb, nvars, vars)
      filename = strsplit(datasets[ii], ".csv")[[1]]
      saveRDS(mbl_pcmb, paste0("../", model, "/pcmb/", filename, ".rds")) # save learned mb as .rds
      saveRDS(mbl_iamb, paste0("../", model, "/iamb_cpp/", filename, ".rds")) # save learned mb as .rds
      file.remove("output.txt")
      
    }
  }
}

setwd("~/Documents/PhDProjects/RStudioProjects/lglbnlearn/")
```


```{r}
parsePCMB = function(output, nvars, vars) {
  
  mbl = list()
  
  for (i in 1:nvars) {
    
    temp = output[i + 1]
    ind = strsplit(temp, "mb:")[[1]][2]
    if (!is.na(ind)) {
      ind = as.numeric(strsplit(ind, ",")[[1]])
      ind = ind[!is.na(ind)]
      mbl[[i]] = vars[ind + 1]
    } else {
      mbl[[i]] = vector(length=0)
    }
    
  }
  
  return(mbl)
  
}
```


# SLL in cpp
```{r}
setwd("~/Documents/Experiments/kdd_exp/SLL/")
dir = "~/Documents/Experiments/kdd_exp/"
model = "30_5_4_1"
#n = 500
#for (model in c("alarm", "barley", "insurance")) {
  for (n in c(10000)) {
    
    datasets = list.files(paste0(dir, "/", model, "/data_csv/", n))
    #file.remove("output.txt")
    for (ii in 1:length(datasets)) {
      
      data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
      vars = colnames(data)
      nvars = ncol(data)
      data = factor2numeric(data) # for artificial models
      write.table(data, paste0(model, ".dat"), row.names = F, col.names = F)
      # copying data into pcmb directory, overwrite the previous data
      # file.copy(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]), paste0(model, ".dat"), overwrite = TRUE) 
      system(paste0("./sll ", model, ".dat -a sll-mb -t all"))
      res_sll = read.table("mb.out", sep = "\n")
      mbl_sll = list()
      for (i in 1:nvars) {
        temp = strsplit(as.character(res_sll[i, 1]), ": ")[[1]][-1]
        if (length(temp) > 0) {
          ind = as.numeric(strsplit(temp, " ")[[1]])  
          mbl_sll[[i]] = vars[ind + 1]
        } else {
          mbl_sll[[i]] = vector(length = 0)
        }
      }
      
      filename = strsplit(datasets[ii], ".csv")[[1]]
      #filename = paste0(c("sll", strsplit(filename, "_")[[1]][-1]), collapse = "_")
      saveRDS(mbl_sll, paste0("../", model, "/sll/", filename, ".rds")) # save learned mb as .rds
      #file.remove("output.txt")
      
    }
    
  }
#}

setwd("~/Documents/PhDProjects/RStudioProjects/lglbnlearn/")
```


## Evaluating on real models
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
for (model in c("alarm")) {

  dag = readRDS(paste0(dir, model, "/dag/", model, ".rds"))
  vars = bnlearn::nodes(dag)
  nvars = length(vars)
  mbt = lapply(vars, bnlearn::mb, x = dag)
  amb = mean(sapply(mbt, length))
  m = matrix(0, 3, 15)
  m[, 1] = model
  m[, 2] = round(amb, 1)
  m[, 3] = c(500, 1000, 5000)
  l = 4
  
  for (folder in c("mml_cpt", "mml_nb", "mml_rand", "iamb_cpp", "pcmb", "sll")) {
    
    for (n in c(500, 1000, 5000)) {
      
      if (n == 1000) {
        ind = 2
      } else if (n == 5000) {
        ind = 3
      } else if (n == 500) {
        ind = 1
      }
         
      learned = list.files(paste0(dir, model, "/", folder), paste0("_s", n, "_"))
      edList = rep(list(rep(0, nvars)), length(learned)) # edit distance
      retList = rep(list(c()), length(learned)) # precision and recall
      
      for (j in 1:length(learned)) {
        
        mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
        
        if (folder %in% c("mml_cpt", "mml_rand")) {
          mbl = symmetry_correction(vars, mbl, "AND")
        } else if (folder %in% c("mml_nb")) {
          mbl = symmetry_correction(vars, mbl, "OR")
        }
        
        for (k in 1:nvars) {
          edList[[j]][k] = mb_false_finding(mbt[[k]], mbl[[k]])
          retList[[j]] = c(retList[[j]], round(mb_retrieval(mbt[[k]], mbl[[k]], nvars), 1))
        }
        
      }# end for j 
      
      avg_ed_over_all_nodes = paste0(round(conf_int(unlist(edList)), 1), collapse = "+-")
      retVec = unlist(retList)
      avg_pre_over_all_nodes = paste0(round(conf_int(retVec[!even(1:length(retVec))]), 2), collapse = "+-")
      avg_rec_over_all_nodes = paste0(round(conf_int(retVec[even(1:length(retVec))]), 2), collapse = "+-")
      avg_pre_rec_over_all_nodes = paste(avg_pre_over_all_nodes, avg_rec_over_all_nodes)
      m[ind, l] = avg_ed_over_all_nodes
      m[ind, l + 6] = avg_pre_rec_over_all_nodes
    }# end for n
    l = l + 1
  }# end for each method
  
  #write.table(m, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)

}

```


## Evaluating for artificial models
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
model = "50_5_4_1"
true = list.files(paste0(dir, model, "/mb_true"))
dags = list.files(paste0(dir, model, "/dag"))
#dag = readRDS(paste0(dir, model, "/dag/", dags[1]))

m = matrix(0, 4, 15)
m[, 1] = model
amb = c() # average mb size
for (x in true) {
  
  mbt = readRDS(paste0(dir, model, "/mb_true/", x))
  amb = c(amb, round(sum(sapply(mbt, length)) / length(mbt), 1))
  
}
m[, 2] = mean(amb)
m[, 3] = c(100, 500, 2000, 5000) 
l = 4

for (folder in c("mml_cpt", "mml_nb", "mml_rand", "iamb_cpp", "iamb_bnlearn", "sll")) {
  
  for (n in c(100, 500, 2000, 5000)) {

    if (n == 2000) {
      ind = 3
    } else if (n == 5000) {
      ind = 4
    } else if (n == 500) {
      ind = 2
    } else if (n == 100) {
      ind = 1
    } 
       
    learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
    datasets = list.files(paste0(dir, model, "/data_rds"), pattern = paste0("_", n, "_"))
    edList = rep(list(rep(0, length(mbt))), length(learned)) # edit distance
    retList = rep(list(c()), length(learned)) # precision and recall
    for (i in 1:length(true)) {
      dag = readRDS(paste0(dir, model, "/dag/", dags[i]))
      vars = bnlearn::nodes(dag)
      mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
      for (j in ((5 * (i - 1)) + 1):(5 * i)) {
        
        #data = readRDS(paste0(dir, model, "/data_rds/", datasets[j]))
        mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
        
        if (folder %in% c("mml_cpt", "mml_rand")) {
          mbl = symmetry_correction(vars, mbl, "AND")
        } else if (folder %in% c("mml_nb")) {
          mbl = symmetry_correction(vars, mbl, "OR")
        }
        
        for (k in 1:length(mbt)) {
          edList[[j]][k] = mb_false_finding(mbt[[k]], mbl[[k]])
          #cat(edList[[j]][k], ":")
          retList[[j]] = c(retList[[j]], round(mb_retrieval(mbt[[k]], mbl[[k]], length(mbt)), 1))
          #cat(round(mb_retrieval(mbt[[k]], mbl[[k]], length(mbt)), 1), "\n")
        }
      }# end for j 
      
    }# end for i 
   
    avg_ed_over_all_nodes = paste0(round(conf_int(unlist(edList)), 1), collapse = "+-")
    retVec = unlist(retList)
    avg_pre_over_all_nodes = paste0(round(conf_int(retVec[!even(1:length(retVec))]), 2), collapse = "+-")
    avg_rec_over_all_nodes = paste0(round(conf_int(retVec[even(1:length(retVec))]), 2), collapse = "+-")
    avg_pre_rec_over_all_nodes = paste(avg_pre_over_all_nodes, avg_rec_over_all_nodes)
    m[ind, l] = avg_ed_over_all_nodes
    m[ind, l + 6] = avg_pre_rec_over_all_nodes
  }# end for n
  l = l + 1
}# end for each method

#write.table(m, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)
```


# evaluation according to mb size increasing
# also evaluate according to the number of correct mb findings, since this could be another factor that has impact on global learning later on
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
nvars = 30
model = paste0(nvars, "_5_4_1")
true = list.files(paste0(dir, model, "/mb_true"))
dags = list.files(paste0(dir, model, "/dag"))

for (iii in 1:length(true)) {
  
  dag = readRDS(paste0(dir, model, "/dag/", dags[iii]))
  vars = bnlearn::nodes(dag)
  mbt = readRDS(paste0(dir, model, "/mb_true/", true[iii]))
  mbt_len = sapply(mbt, length)
  mbt_order = order(mbt_len)
  mbt_len_ordered = mbt_len[mbt_order]
  
  for (n in c(500, 2000, 5000)) {
    m = matrix(0, length(unique(mbt_len_ordered)), 7)
    m[,1] = unique(mbt_len_ordered)
    w = m
    for (ii in 1:6) {
      
      folders = c("mml_cpt", "mml_nb", "mml_rand", "iamb_cpp", "pcmb", "sll")
      folder = folders[ii]
      learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
      edmtx = matrix(0, 5, length(mbt)) # edit distance
      jj = 1
      for (j in ((5 * (iii - 1)) + 1):(5 * iii)) {
        mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
        if (folder %in% c("mml_cpt", "mml_nb", "mml_rand")) mbl = symmetry_correction(vars, mbl, "AND")
        for (k in 1:length(mbt)) edmtx[jj, k] = mb_false_finding(mbt[[k]], mbl[[k]])
        jj = jj + 1
      }
      for (i in 1:nrow(m)) {
        indices = which(mbt_len == m[i, 1])
        temp = as.vector(edmtx[, indices])
        avg_ed = round(mean(temp), 1)
        error = round(sd(temp) * 1.96 / sqrt(length(temp)), 1)
        m[i, ii + 1] = avg_ed
        w[i, ii + 1] = error
      }
      
    }
    
    colnames(m) = colnames(w) = c("nmb", "cpt", "nb", "random", "iamb", "pcmb", "sll")
    error = as.vector(w[, -1])
    m = data.frame(m)
    m_melt = melt(m, id = "nmb")
    colnames(m_melt)[2] = "alg"
    #meltTempMean$Samples = factor(meltTempMean$Samples, levels = rev(levels(meltTempMean$Samples)))
    figure = ggplot(m_melt, aes(x = nmb, y = value, group = alg, colour = alg)) + 
      ylab(label = "edit dist") + xlab("mb size") + geom_line(aes(linetype = alg)) + geom_point(aes(shape = alg)) + 
      ylim(0, 30) + xlim(0, 25) + guides(linetype = guide_legend()) +
      theme(legend.key.width = unit(1.5, "cm")) +
      geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
      
    #figure
    filename = paste0(strsplit(true[iii], ".rds")[[1]], "_", nvars, "_", n, collapse = "")
    ggsave(filename, device = "png", path = "~/Documents/Journal KDD/figures/")
  } # end for n
}
```


## plotting edit distance vs. mb size
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
nvars = 50
model = paste0(nvars, "_5_4_1")
true = list.files(paste0(dir, model, "/mb_true"))
dags = list.files(paste0(dir, model, "/dag"))
mbt_len_mtx = matrix(0, 5, nvars)
for (i in 1:length(dags)) {
  mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
  mbt_len_mtx[i, ] = sapply(mbt, length)
}

for (n in c(100, 500, 2000, 5000)) {
  
#n = 2000
mbt_len_unique = unique(as.vector(mbt_len_mtx))
m = matrix(0, length(mbt_len_unique), 7)
m[, 1] = mbt_len_unique[order(mbt_len_unique)]
w = m

for (ii in 1:6) {
  folders = c("mml_cpt", "mml_nb", "mml_rand", "iamb_cpp", "pcmb", "sll")
  folder = folders[ii]
  ll = rep(list(c()), nrow(m))
  learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
  for (i in 1:5) {
    dag = readRDS(paste0(dir, model, "/dag/", dags[i]))
    vars = bnlearn::nodes(dag)
    mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
    #edmtx = matrix(0, 5, length(mbt)) # edit distance
    #jj = 1
    for (j in ((5 * (i - 1)) + 1):(5 * i)) {
      mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
      if (folder %in% c("mml_cpt", "mml_nb", "mml_rand")) mbl = symmetry_correction(vars, mbl, "AND")
      for (k in 1:length(mbt)) {
        ind = which(m[,1] == length(mbt[[k]]))
        ll[[ind]] = c(ll[[ind]], mb_false_finding(mbt[[k]], mbl[[k]]))
        # m[ind, ii + 1] = m[ind, ii + 1] + mb_false_finding(mbt[[k]], mbl[[k]])
        # w[ind, ii + 1] = w[ind, ii + 1] + 1
      }
      #jj = jj + 1
    }
    #write.table(edmtx, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)
    #ll[[i]] = edmtx
  }
  m[, ii + 1] = sapply(ll, mean)
  w[, ii + 1] = 1.96 * sapply(ll, sd) / sqrt(sapply(ll, length))
  
}

m = round(m, 1)
w = round(w, 1)
colnames(m) = colnames(w) = c("nmb", "MML_CPT", "MML_NB", "MBMML_MBP", "IAMB", "PCMB", "SLL")
m = m[, c(1:4, 7, 5, 6)]
m = data.frame(m)
m_melt = melt(m, id = "nmb")
colnames(m_melt)[2] = "Algorithm"
error = as.vector(w[, -1])
figure = ggplot(m_melt, aes(x = nmb, y = value, group = Algorithm, colour = Algorithm)) + 
  ylab(label = "Edit distance") + xlab("Markov blanket size") + geom_line(aes(linetype = Algorithm)) + geom_point(aes(shape = Algorithm)) + scale_colour_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) + 
  ylim(0, ceiling(max(m_melt$value+error))) + xlim(0, max(mbt_len_mtx)) + guides(linetype = guide_legend()) +
  theme(legend.key.width = unit(1.5, "cm")) +
  geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
figure
filename = paste0("ed_vs_mbsize_", nvars, "_5_4_1_", n, collapse = "")
filename = paste0(filename, ".pdf", collapse = "")
ggsave(filename, device = "pdf", path = "~/Documents/Papers during PhD/Journal paper on MB discovery/figures", width = 7.29, height = 4.5, units = "in")

}
```


## Sym enforcement using mml (no better than deterministic sym enforcement)
```{r}
m = matrix(0, 10, 3)
for (k in 1:10) {
dag = randDag(10, 3)
cpts = randCPTs(dag, 4, 1)
data = rbn(cpts, 500)
vars = names(data)
n = nrow(data)
arities = sapply(data, nlevels)
names(arities) = c()
varCnt = count_occurance(data, arities)
data = data.matrix(data)
mbt = lapply(vars, bnlearn::mb, x = dag)
mbl = lapply(vars, forward_greedy_fast, data = data, varCnt = varCnt, arities = arities, vars = vars, sampleSize = n)
mbDiscrepancy = mb_discrepancy(vars, mbl)
if (nrow(mbDiscrepancy) > 0) {
  # cat("dis exist! \n")
  mbl_sym = sym_enforcement_mml(vars, varCnt, arities, n, mbl, mbDiscrepancy)
  mbl_sym2 = symmetry_correction(vars, mbl, "AND")
  #mb_discrepancy(vars, mbl_sym)
  ed = ed_sym = ed_sym2 = c()
  for (i in 1:ncol(data)) {
    ed = c(ed, mb_false_finding(mbt[[i]], mbl[[i]]))
    ed_sym = c(ed_sym, mb_false_finding(mbt[[i]], mbl_sym[[i]]))
    ed_sym2 = c(ed_sym2, mb_false_finding(mbt[[i]], mbl_sym2[[i]]))
  }
  # cat(mean(ed), "\n")
  # cat(mean(ed_sym), "\n")
  # cat(mean(ed_sym2), "\n")
}
m[k, ] = c(mean(ed), mean(ed_sym), mean(ed_sym2))
}

```


## plotting edit distance vs. sample size 
```{r}
model = "30_5_4_1"
if (model == "30_5_4_1") {
# 30_5_4_1
x = c(7.5,7.3,7.2,7.5,8.7,7.4,4.5,6.2,4.6,6.2,5.7,6.3,3.3,5.1,3.5,5.2,3.5,4.4,2.6,4.5,3,4.7,2.6,2.9)
error = c(0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.2)
} else if (model == "50_5_4_1") {
# 50_5_4_1
x = c(10.6,9.7,10.4,9.5,12.6,9.3,6.4,7.9,6.5,8.2,7.5,8,5.1,6.8,5.2,7.2,5.5,6.2,4.2,6,4.5,6.5,4.4,4.4)
error = c(0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.2)
} else if (model == "child") {
# child
x = c(0.9, 0.9, 1.4, 1.7, 1.4, 1, 0.7, 0.7, 0.9, 1.6, 1, 0.8, 0.5, 0.6, 0.2, 1.4, 0.2, 0.2)
error = c(0.2, 0.2, 0.2,0.2,0.2,0.2, 0.1,0.2,0.2,0.2,0.2,0.1, 0.1,0.1,0.1,0.2,0.1,0.1)
} else if (model == "insurance") {
# insurance
x = c(3.3,3.5,3.7,3.6,3.4,3.1,2.9,3.3,3.1,3.7,3,2.7,2.1,2.8,2.4,2.8,1.8,2)
error = c(0.2,0.2,0.3,0.3,0.2,0.2,0.2,0.2,0.3,0.3,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2)
} else if (model == "alarm") {
# alarm
x = c(1.4,2.1,2.3,2.2,1.7,0.8,1,1.8,1.9,2,1.1,0.6,0.5,1.5,1.5,1.5,0.2,0.2)
error = c(.1,.2,.2,.2,.2,.1,.1,.2,.2,.2,.1,.1,.1,.2,.1,.2,.1,0)
} else if (model == "barley") {
# barley
x = c(4,4.1,4.4,4.9,11.1,4.2,3.7,3.8,4.2,5,NA,3.8,3.4,3.6,3.5,4.3,NA,3.1)
error = c(.3,.3,.3,.3,.5,.2,.3,.3,.3,.3,NA,.2,.3,.3,.3,.3,NA,.2)
} else if (model == "hailfinder") {
# hailfinder
x = c(4.4,4.3,5.2,4.2,7.6,4.3,4.4,4.3,5,4.5,6.7,4.1,4.3,4.3,5.1,5.1,3.9,4)
error = c(.3,.2,.3,.2,.5,.3,.3,.2,.3,.2,.4,.3,.3,.2,.3,.2,.2,.3)
}

if (model %in% c("30_5_4_1", "50_5_4_1")) {
m = matrix(x, 4, 6, byrow = T)
m = cbind((c(100, 500, 2000, 5000)), m)
colnames(m) = c("nmb", "MBMML_CPT", "MBMML_NB", "MBMML_MBP", "IAMB", "PCMB", "SLL")
m = m[,c(1:4,7,5,6)]
} else {
m = matrix(x, 3, 6, byrow = T)
m = cbind((c(500, 1000, 5000)), m)
colnames(m) = c("nmb", "MBMML_CPT", "MBMML_NB", "MBMML_MBP", "IAMB", "PCMB", "SLL")
m = m[,c(1:4,7,5,6)]
}

m = data.frame(m)
m_melt = melt(m, id = "nmb")
colnames(m_melt)[2] = "Algorithm"
figure = ggplot(m_melt, aes(x = nmb, y = value, group = Algorithm, colour = Algorithm)) + 
ylab(label = "Edit distance") + xlab("Sample size") + geom_line(aes(linetype = Algorithm)) + geom_point(aes(shape = Algorithm)) + scale_colour_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#CC79A7")) +
ylim(floor(min(m_melt$value-error)), ceiling(max(m_melt$value+error))) + xlim(100, 5000) + guides(linetype = guide_legend()) +
theme(legend.key.width = unit(1.5, "cm")) +
geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
figure
filename = paste0("ed_vs_samplesize_", model, ".pdf", collapse = "")
ggsave(filename, device = "pdf", path = "~/Documents/Papers during PhD/Journal paper on MB discovery/figures", width = 7.29, height = 4.5, units = "in")
```


## MML w/ wrong prior
for alpha < 1 close to 0, all priors performs similarly; for priors > 1, such as 100, 200, alpha=1 is the best, even better than using true prior; for alpha=1, alpha=1 is the best.
```{r}
# dag = randDag(10, 3)
# graphviz.plot(dag)
dag = readRDS("~/Documents/Experiments/kdd_exp/30_5_4_1/dag/dag_172007.rds")
nvars = bnlearn::nnodes(dag)
#vv = c(seq(0.1, 1, 0.3), seq(10, 100, 30))
vv = c(0.1, 0.4, 0.7, 1, 10, 40, 70, 100)
#df = c()
#beta = 0.1
n = 500
itr = 2
df0 = df3 = matrix(0, length(vv) * itr * nvars, 5)
# e0 = e1 = e2 = e3 = rep(0, length(vv) * itr * nvars)
# acc0 = acc3 = matrix(0, length(vv) * itr * nvars, 2)
iii = 1
no_cores = 3
registerDoParallel(no_cores)
for (beta in vv) {
  cat(beta, "\n")
  cnt = matrix(0, itr, 2)
  
for (rr in 1:itr) {
  
cat(rr, "\n")
cpts = randCPTs(dag, 4, beta)
data = rbn(cpts, n)
vars = colnames(data)
nvars = length(vars)
arities = sapply(data, nlevels)
varCnt = di = count_occurance(data, arities)
mbt = lapply(vars, bnlearn::mb, x = dag)

l0 = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar% {
    forward_greedy_fast(data, varCnt, arities, vars, n, target, alpha=1)
      }

l3 = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar% {
        targetIndex = which(vars == target)
        alpha3 = rep(beta, arities[targetIndex])
    forward_greedy_fast(data, varCnt, arities, vars, n, target, alpha=alpha3)
      }

for (i in 1:nvars) {
  df0[iii, 1] = mb_false_finding(mbt[[i]], l0[[i]])
  df0[iii, 2:3] = mb_retrieval(mbt[[i]], l0[[i]], nvars)
  df0[iii, 4] = beta
  df3[iii, 1] = mb_false_finding(mbt[[i]], l3[[i]])
  df3[iii, 2:3] = mb_retrieval(mbt[[i]], l3[[i]], nvars)
  df3[iii, 4] = beta
  df0[iii, 5] = sum(sapply(l0, length))
  df3[iii, 5] = sum(sapply(l3, length))
  iii = iii + 1
}

# v0 = c(v0, e0)
# v3 = c(v3, e3)
# cnt[rr, ] = c(mean(e0), mean(e3))

}# end for each itr
# df = c(df, colMeans(cnt))
}# end for each prior

stopImplicitCluster()

m0 = m3 = s0 = s3 = matrix(0, length(vv), ncol(df0))
a = nrow(df0)/length(vv)
for (i in 1:length(vv)) {
  m0[i,] = apply(df0[((i-1)*a + 1):(i*a),], 2, mean)
  s0[i,] = apply(df0[((i-1)*a + 1):(i*a),], 2, sd)
  m3[i,] = apply(df3[((i-1)*a + 1):(i*a),], 2, mean)
  s3[i,] = apply(df3[((i-1)*a + 1):(i*a),], 2, sd)
}
```

# plot
```{r}
df = cbind(m0[,1], m3[,1], log(m0[,4]))
colnames(df) = c("uniform", "true", "prior")
df = as.data.frame(df)
m_melt = melt(df, "prior")
error = c(s0[,1], s3[,1])
error = 1.96 * error / sqrt(length(a))
figure = ggplot(m_melt, aes(x = prior, y = value, group = variable, colour = variable))+ylab(label = "Edit distance") + xlab("Log(True prior)") + geom_line(aes(linetype = variable)) + geom_point(aes(shape = variable))+
guides(linetype = guide_legend()) +
theme(legend.key.width = unit(1.5, "cm")) + ylim(0, 10) + xlim(-3, 5)
figure
# + 
# geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
# figure
# + 
# filename = "ed_vs_trueprior_30_5_4_alpha_134445_5000_v2.pdf"
# ggsave(filename, device = "pdf", path = "~/Documents/Journal KDD/figures/", width = 7.29, height = 4.5, units = "in")
```


## mean mb size
```{r}
files = list.files("~/Documents/Experiments/kdd_exp/50_5_4_1/dag/")
ss = 0
for (i in 1:length(files)) {
  dag = readRDS(paste0("~/Documents/Experiments/kdd_exp/50_5_4_1/dag/", files[i]))
  vars = bnlearn::nodes(dag)
  ss = ss + mean(sapply(sapply(vars, bnlearn::mb, x = dag), length))
}
ss/length(files)

```









