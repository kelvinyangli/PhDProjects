---
title: "mb_discovery_comparison_for_journal_paper_part1"
author: "kl"
date: "31 January 2018"
output: html_document
---

```{r global_options, include=F, warning=F, eval=F, echo=F, message=F}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE, echo = F, eval = T)
if (.Platform$OS.type == "windows") {
  dir = "C:/Users/"
} else {
  dir = "/home/"
}
source(paste0(dir, "kl/Documents/PhDProjects/RStudioProjects/local2global/scripts/load_libraries.R"))

```

These exp take 5 parameters into account, ordered by priority: 

* nvars 10, 30, 50, 100
* n 50, 500, 5000
* npa 2, 4, 6
* narity 2, 3, 4
* beta (derichlet concentration parameter) 1, 5, 25

```{r}
dir = "~/Documents/Experiments/kdd_exp/"
vnvars = c(10, 50, 100)
vnpa = c(2, 4, 6)
vnarity = c(2, 3, 4)
vbeta = c(1, 5, 25)
vn = c(50, 500, 5000)
for (nvars in vnvars) {
  
  for (npa in vnpa) {
          
    for (narity in vnarity) {
      
      for (beta in vbeta) {
        nvars = 50
        npa = narity = 2 
        beta = 1
        dirname = paste(nvars, npa, narity, beta, sep = "_")
        
        for (itr in 1:5) {
  
          sd = randSeed()
          set.seed(sd)
          dag = randDag(nvars, npa)
          cpts = randCPTs(dag, narity, beta)
          nodes = bnlearn::nodes(dag)
          mbtrue = list()
          i = 1
          for (x in nodes) {
            mbtrue[[i]] = bnlearn::mb(dag, x)
            i = i + 1
          }
          saveRDS(dag, paste0(dir, dirname, "/dag/dag_", sd, ".rds"))
          saveRDS(cpts, paste0(dir, dirname, "/cpts/cpt_", sd, ".rds"))
          saveRDS(mbtrue, paste0(dir, dirname, "/mb_true/mb_", sd, ".rds"))
          
          for (n in vn) {
            
            datasd = randSeed()
            set.seed(datasd)
            data = rbn(cpts, n)  
            saveRDS(data, paste0(dir, dirname, "/data_rds/data_", n, "_", sd, "_", datasd, ".rds"))
            write.csv(data, paste0(dir, dirname, "/data_csv/data_", n, "_", sd, "_", datasd, ".csv"), row.names = F)
            
          }
        }# end itr
      }# end beta
    }
  }
}

```


## mml + cpt, nb, rand learning using parallel computing on real models 
```{r}
model = "child"
dir = "~/Documents/Experiments/kdd_exp/"
n = 5000
datasets = list.files(paste0(dir, model, "/data_csv/", n))

### starting parallel computing ###
no_cores = 4
registerDoParallel(no_cores)
for (ii in 1:length(datasets)) {
  filename = strsplit(datasets[ii], ".csv")[[1]]
  mbcpt = mbnb = mbrandv = list()
  data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii])) 
  vars = colnames(data)
  nvars = length(vars)
  data_cat = numeric2categorical(data)
  arities = sapply(data_cat, nlevels)
  di = count_occurance(data_cat, arities)
  #data = data.matrix(data_cat)

  alpha_est = list()
  ls = list()
  for (i in 1:nvars) {
    ls[[i]] = freqs(table(data[, i]))
  }
  k = 1
  for (i in unique(arities)) {
    if (i > 1) {
      par = data.frame()
      ind = which(arities == i)
      if (length(ind) > 0) {
        if (length(ind) == 1) {
          # make a fake para to have at least 2 rows
          temp = ls[[ind]] + 0.001
          temp = temp / sum(temp)
          par = rbind(ls[[ind]],  temp)
        } else {
          for (j in ind) {
          par = rbind(par, ls[[j]])
          }
          
        }# end else 
        alpha_est[[k]] = dirichlet.mle(par)$alpha
      }
      k = k + 1
    }
  }

  mbcpt = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar% {
    targetIndex = which(vars == target)
    alpha = alpha_est[[which(sapply(alpha_est, length) == arities[targetIndex])]]
    forward_greedy_fast(data, di, arities, vars, n, target, alpha)
      }
  saveRDS(mbcpt, paste0(dir, model, "/mml_cpt_est_prior/", filename, ".rds"))

  # mbnb = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "nb")
  # saveRDS(mbnb, paste0(dir, model, "/mml_nb/", filename, ".rds"))
  # 
  # mbrand = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "random", varCnt = di, prior = "uniform")
  # saveRDS(mbrand, paste0(dir, model, "/mml_rand/", filename, ".rds"))

}

stopImplicitCluster()
```


## mml + cpt, nb, rand learning using parallel computing on artificial models
```{r}
# library(parallel)
# library(foreach)
# library(doParallel)
dir = "~/Documents/Experiments/kdd_exp/"
model = "30_5_4_1"
n = 5000
datasets = list.files(paste0(dir, model, "/data_rds/", n))
#datasets = list.files(paste0(dir, model, "/data_rds/", n))
#dags = list.files(paste0(dir, params, "/dag/"))

## starting parallel computing ###
no_cores = 4
registerDoParallel(no_cores)
for (ii in 1:length(datasets)) {
  #cat(i, "\n")
  #data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
  data = readRDS(paste0(dir, model, "/data_rds/", n, "/", datasets[ii]))
  filename = strsplit(datasets[ii], ".rds")[[1]]
  nvars = ncol(data)
  vars = names(data)
  #data_cat = numeric2categorical(data)
  arities = sapply(data, nlevels)
  varCnt = count_occurance(data, arities)
  
  alpha_est = list()
  ls = list()
  for (i in 1:nvars) {
    ls[[i]] = freqs(table(data[, i]))
  }
  k = 1
  for (i in unique(arities)) {
    if (i > 1) {
      par = data.frame()
      ind = which(arities == i)
      if (length(ind) > 0) {
        for (j in ind) {
          par = rbind(par, ls[[j]])
        }
        alpha_est[[k]] = dirichlet.mle(par)$alpha
      }
      k = k + 1
    }
  }
  
  #data = data.matrix(data)
  
  # for (target in vars) {
  #   targetIndex = which(vars == target)
  #   alpha = alpha_est[[which(sapply(alpha_est, length) == arities[targetIndex])]]
  #   forward_greedy_fast(data, varCnt, arities, vars, n, target, alpha, debug = F)
  # }
  
  mbcpt = foreach(target = vars,
      .combine = list,
      .multicombine = TRUE) %dopar% {
    targetIndex = which(vars == target)
    alpha = alpha_est[[which(sapply(alpha_est, length) == arities[targetIndex])]]
    forward_greedy_fast(data, varCnt, arities, vars, n, target, alpha)
      }
  
  # mbnb = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "nb")

  # mbrand = foreach(target = vars,
  #     .combine = list,
  #     .multicombine = TRUE) %dopar%
  #   forward_greedy(data, arities, vars, n, target, "random", varCnt = varCnt, prior = "uniform")
  
  saveRDS(mbcpt, paste0(dir, model, "/mml_cpt_est_prior/cpt_", filename))
  # saveRDS(mbnb, paste0(dir, model, "/nb/nb_", filename))
  # saveRDS(mbrand, paste0(dir, model, "/rand/rand_", filename))
  
}

stopImplicitCluster()
```


## pcmb and iamb in cpp on artificial models
consider alpha = 0.05 for pcmb and iamb (because stick to the original paper's exp setting in tsamardinos and sll)
```{r}
setwd("~/Documents/Experiments/kdd_exp/PCMB/")
dir = "~/Documents/Experiments/kdd_exp/"
for (model in c("30_5_4_1", "50_5_4_1")) {
  for (n in c(500, 2000, 5000)) {
    datasets = list.files(paste0(dir, "/", model, "/data_csv/", n))
    #file.remove("output.txt")
    for (ii in 1:length(datasets)) {
      
      data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
      vars = colnames(data)
      nvars = ncol(data)
      # copying data into pcmb directory, overwrite the previous data
      file.copy(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]), paste0(model, ".data"), overwrite = TRUE) 
      res_pcmb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 1 0.05"), intern = TRUE)
      res_iamb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 0 0.05"), intern = TRUE)
      mbl_pcmb = parsePCMB(res_pcmb, nvars, vars)
      mbl_iamb = parsePCMB(res_iamb, nvars, vars)
      filename = strsplit(datasets[ii], ".csv")[[1]]
      
      saveRDS(mbl_pcmb, paste0("../", model, "/pcmb/", filename, ".rds")) # save learned mb as .rds
      saveRDS(mbl_iamb, paste0("../", model, "/iamb_cpp/", filename, ".rds")) # save learned mb as .rds
      #file.remove("output.txt")
      
    }
  }
}

setwd("~/Documents/PhDProjects/RStudioProjects/lglbnlearn/")
```


## pcmb and iamb in cpp on real models
consider alpha = 0.05 for pcmb and iamb (because stick to the original paper's exp setting in tsamardinos and sll)
```{r}
setwd("~/Documents/Experiments/kdd_exp/PCMB/")
dir = "~/Documents/Experiments/kdd_exp/"
for (model in c("child", "alarm", "barley", "hailfinder", "insurance")) {
  for (n in c(500, 1000, 5000)) {
    datasets = list.files(paste0(dir, "/", model, "/data_csv/", n))
    #file.remove("output.txt")
    for (ii in 1:length(datasets)) {
      
      data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
      vars = colnames(data)
      nvars = ncol(data)
      # copying data into pcmb directory, overwrite the previous data
      file.copy(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]), paste0(model, ".data"), overwrite = TRUE) 
      res_pcmb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 1 0.05"), intern = TRUE)
      res_iamb = system(paste0("./kmb4_linux ", model, ".data ", n, " ", nvars, " -1 1.0 0 0 0.05"), intern = TRUE)
      mbl_pcmb = parsePCMB(res_pcmb, nvars, vars)
      mbl_iamb = parsePCMB(res_iamb, nvars, vars)
      filename = strsplit(datasets[ii], ".csv")[[1]]
      saveRDS(mbl_pcmb, paste0("../", model, "/pcmb/", filename, ".rds")) # save learned mb as .rds
      saveRDS(mbl_iamb, paste0("../", model, "/iamb_cpp/", filename, ".rds")) # save learned mb as .rds
      #file.remove("output.txt")
      
    }
  }
}

setwd("~/Documents/PhDProjects/RStudioProjects/lglbnlearn/")
```


```{r}
parsePCMB = function(output, nvars, vars) {
  
  mbl = list()
  
  for (i in 1:nvars) {
    
    temp = output[i + 1]
    ind = strsplit(temp, "mb:")[[1]][2]
    if (!is.na(ind)) {
      ind = as.numeric(strsplit(ind, ",")[[1]])
      ind = ind[!is.na(ind)]
      mbl[[i]] = vars[ind + 1]
    } else {
      mbl[[i]] = vector(length=0)
    }
    
  }
  
  return(mbl)
  
}
```


# SLL in cpp
```{r}
setwd("~/Documents/Experiments/kdd_exp/SLL/")
dir = "~/Documents/Experiments/kdd_exp/"
model = "30_5_4_1"
#n = 500
#for (model in c("alarm", "barley", "insurance")) {
  for (n in c(5000)) {
    
    datasets = list.files(paste0(dir, "/", model, "/data_csv/", n))
    #file.remove("output.txt")
    for (ii in 1:length(datasets)) {
      
      data = read.csv(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]))
      vars = colnames(data)
      nvars = ncol(data)
      data = factor2numeric(data) # for artificial models
      write.table(data, paste0(model, ".dat"), row.names = F, col.names = F)
      # copying data into pcmb directory, overwrite the previous data
      # file.copy(paste0(dir, model, "/data_csv/", n, "/", datasets[ii]), paste0(model, ".dat"), overwrite = TRUE) 
      system(paste0("./sll ", model, ".dat -a sll-mb -t all"))
      res_sll = read.table("mb.out", sep = "\n")
      mbl_sll = list()
      for (i in 1:nvars) {
        temp = strsplit(as.character(res_sll[i, 1]), ": ")[[1]][-1]
        if (length(temp) > 0) {
          ind = as.numeric(strsplit(temp, " ")[[1]])  
          mbl_sll[[i]] = vars[ind + 1]
        } else {
          mbl_sll[[i]] = vector(length = 0)
        }
      }
      
      filename = strsplit(datasets[ii], ".csv")[[1]]
      #filename = paste0(c("sll", strsplit(filename, "_")[[1]][-1]), collapse = "_")
      saveRDS(mbl_sll, paste0("../", model, "/sll/", filename, ".rds")) # save learned mb as .rds
      #file.remove("output.txt")
      
    }
    
  }
#}

setwd("~/Documents/PhDProjects/RStudioProjects/lglbnlearn/")
```


## Evaluating for real models
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
model = "child"
dag = readRDS(paste0(dir, model, "/dag/", model, ".rds"))
vars = bnlearn::nodes(dag)
nvars = length(vars)
mbt = lapply(vars, bnlearn::mb, x = dag)
amb = mean(sapply(mbt, length))
m = matrix(0, 3, 15)
m[, 1] = model
m[, 2] = round(amb, 1)
m[, 3] = c(500, 1000, 5000)
l = 4

for (folder in c("mml_cpt", "mml_cpt_est_prior", "mml_rand", "iamb_cpp", "pcmb", "sll")) {
  
  for (n in c(500, 1000, 5000)) {
    
    if (n == 1000) {
      ind = 2
    } else if (n == 5000) {
      ind = 3
    } else if (n == 500) {
      ind = 1
    }
       
    learned = list.files(paste0(dir, model, "/", folder), paste0("_s", n, "_"))
    edList = rep(list(rep(0, nvars)), length(learned)) # edit distance
    retList = rep(list(c()), length(learned)) # precision and recall
    
    for (j in 1:length(learned)) {
      
      mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
      if (folder %in% c("mml_cpt", "mml_nb", "mml_rand")) mbl = symmetry_correction(vars, mbl, "AND")
      for (k in 1:nvars) {
        edList[[j]][k] = mb_false_finding(mbt[[k]], mbl[[k]])
        retList[[j]] = c(retList[[j]], round(mb_retrieval(mbt[[k]], mbl[[k]], nvars), 1))
      }
      
    }# end for j 
    
    avg_ed_over_all_nodes = paste0(round(conf_int(unlist(edList)), 1), collapse = "+-")
    retVec = unlist(retList)
    avg_pre_over_all_nodes = paste0(round(conf_int(retVec[!even(1:length(retVec))]), 1), collapse = "+-")
    avg_rec_over_all_nodes = paste0(round(conf_int(retVec[even(1:length(retVec))]), 1), collapse = "+-")
    avg_pre_rec_over_all_nodes = paste(avg_pre_over_all_nodes, avg_rec_over_all_nodes)
    m[ind, l] = avg_ed_over_all_nodes
    m[ind, l + 6] = avg_pre_rec_over_all_nodes
  }# end for n
  l = l + 1
}# end for each method
#write.table(m, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)

```


## Evaluating for artificial models
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
model = "30_5_4_1"
true = list.files(paste0(dir, model, "/mb_true"))
dags = list.files(paste0(dir, model, "/dag"))
#dag = readRDS(paste0(dir, model, "/dag/", dags[1]))

m = matrix(0, 3, 15)
m[, 1] = model
amb = c() # average mb size
for (x in true) {
  
  mbt = readRDS(paste0(dir, model, "/mb_true/", x))
  amb = c(amb, round(sum(sapply(mbt, length)) / length(mbt), 1))
  
}
m[, 2] = mean(amb)
m[, 3] = c(500, 2000, 5000) 
l = 4

for (folder in c("mml_cpt", "mml_cpt_est_prior", "mml_rand", "iamb_cpp", "pcmb", "sll")) {
  
  for (n in c(500, 2000, 5000)) {

    if (n == 2000) {
      ind = 2
    } else if (n == 5000) {
      ind = 3
    } else if (n == 500) {
      ind = 1
    }
       
    learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
    datasets = list.files(paste0(dir, model, "/data_rds"), pattern = paste0("_", n, "_"))
    edList = rep(list(rep(0, length(mbt))), length(learned)) # edit distance
    retList = rep(list(c()), length(learned)) # precision and recall
    for (i in 1:length(true)) {
      dag = readRDS(paste0(dir, model, "/dag/", dags[i]))
      vars = bnlearn::nodes(dag)
      mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
      for (j in ((5 * (i - 1)) + 1):(5 * i)) {
        
        #data = readRDS(paste0(dir, model, "/data_rds/", datasets[j]))
        mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
        if (folder %in% c("mml_cpt", "mml_cpt_est_prior", "mml_nb", "mml_rand")) mbl = symmetry_correction(vars, mbl, "AND")
        for (k in 1:length(mbt)) {
          edList[[j]][k] = mb_false_finding(mbt[[k]], mbl[[k]])
          #cat(edList[[j]][k], ":")
          retList[[j]] = c(retList[[j]], round(mb_retrieval(mbt[[k]], mbl[[k]], length(mbt)), 1))
          #cat(round(mb_retrieval(mbt[[k]], mbl[[k]], length(mbt)), 1), "\n")
        }
      }# end for j 
      
    }# end for i 
   
    avg_ed_over_all_nodes = paste0(round(conf_int(unlist(edList)), 1), collapse = "+-")
    retVec = unlist(retList)
    avg_pre_over_all_nodes = paste0(round(conf_int(retVec[!even(1:length(retVec))]), 1), collapse = "+-")
    avg_rec_over_all_nodes = paste0(round(conf_int(retVec[even(1:length(retVec))]), 1), collapse = "+-")
    avg_pre_rec_over_all_nodes = paste(avg_pre_over_all_nodes, avg_rec_over_all_nodes)
    m[ind, l] = avg_ed_over_all_nodes
    m[ind, l + 6] = avg_pre_rec_over_all_nodes
  }# end for n
  l = l + 1
}# end for each method

#write.table(m, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)
```


# evaluation according to mb size increasing
# also evaluate according to the number of correct mb findings, since this could be another factor that has impact on global learning later on
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
nvars = 30
model = paste0(nvars, "_5_4_1")
true = list.files(paste0(dir, model, "/mb_true"))
dags = list.files(paste0(dir, model, "/dag"))

for (iii in 1:length(true)) {
  
  dag = readRDS(paste0(dir, model, "/dag/", dags[iii]))
  vars = bnlearn::nodes(dag)
  mbt = readRDS(paste0(dir, model, "/mb_true/", true[iii]))
  mbt_len = sapply(mbt, length)
  mbt_order = order(mbt_len)
  mbt_len_ordered = mbt_len[mbt_order]
  
  for (n in c(500, 2000, 5000)) {
    m = matrix(0, length(unique(mbt_len_ordered)), 7)
    m[,1] = unique(mbt_len_ordered)
    w = m
    for (ii in 1:6) {
      
      folders = c("mml_cpt", "mml_nb", "mml_rand", "iamb_cpp", "pcmb", "sll")
      folder = folders[ii]
      learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
      edmtx = matrix(0, 5, length(mbt)) # edit distance
      jj = 1
      for (j in ((5 * (iii - 1)) + 1):(5 * iii)) {
        mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
        if (folder %in% c("mml_cpt", "mml_nb", "mml_rand")) mbl = symmetry_correction(vars, mbl, "AND")
        for (k in 1:length(mbt)) edmtx[jj, k] = mb_false_finding(mbt[[k]], mbl[[k]])
        jj = jj + 1
      }
      for (i in 1:nrow(m)) {
        indices = which(mbt_len == m[i, 1])
        temp = as.vector(edmtx[, indices])
        avg_ed = round(mean(temp), 1)
        error = round(sd(temp) * 1.96 / sqrt(length(temp)), 1)
        m[i, ii + 1] = avg_ed
        w[i, ii + 1] = error
      }
      
    }
    
    colnames(m) = colnames(w) = c("nmb", "cpt", "nb", "random", "iamb", "pcmb", "sll")
    error = as.vector(w[, -1])
    m = data.frame(m)
    m_melt = melt(m, id = "nmb")
    colnames(m_melt)[2] = "alg"
    #meltTempMean$Samples = factor(meltTempMean$Samples, levels = rev(levels(meltTempMean$Samples)))
    figure = ggplot(m_melt, aes(x = nmb, y = value, group = alg, colour = alg)) + 
      ylab(label = "edit dist") + xlab("mb size") + geom_line(aes(linetype = alg)) + geom_point(aes(shape = alg)) + 
      ylim(0, 30) + xlim(0, 25) + guides(linetype = guide_legend()) +
      theme(legend.key.width = unit(1.5, "cm")) +
      geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
      
    #figure
    filename = paste0(strsplit(true[iii], ".rds")[[1]], "_", nvars, "_", n, collapse = "")
    ggsave(filename, device = "png", path = "~/Documents/Journal KDD/figures/")
  } # end for n
}
```


## mb size ordered version 2
```{r}
dir = "~/Documents/Experiments/kdd_exp/"
nvars = 30
model = paste0(nvars, "_5_4_1")
true = list.files(paste0(dir, model, "/mb_true"))
dags = list.files(paste0(dir, model, "/dag"))
mbt_len_mtx = matrix(0, 5, nvars)
for (i in 1:length(dags)) {
  mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
  mbt_len_mtx[i, ] = sapply(mbt, length)
}

for (n in c(500, 2000, 5000)) {
  
#n = 2000
mbt_len_unique = unique(as.vector(mbt_len_mtx))
m = matrix(0, length(mbt_len_unique), 7)
m[, 1] = mbt_len_unique[order(mbt_len_unique)]
w = m

for (ii in 1:6) {
  folders = c("mml_cpt", "mml_nb", "mml_rand", "iamb_cpp", "pcmb", "sll")
  folder = folders[ii]
  ll = rep(list(c()), nrow(m))
  learned = list.files(paste0(dir, model, "/", folder), pattern = paste0("_", n, "_"))
  for (i in 1:5) {
    dag = readRDS(paste0(dir, model, "/dag/", dags[i]))
    vars = bnlearn::nodes(dag)
    mbt = readRDS(paste0(dir, model, "/mb_true/", true[i]))
    #edmtx = matrix(0, 5, length(mbt)) # edit distance
    #jj = 1
    for (j in ((5 * (i - 1)) + 1):(5 * i)) {
      mbl = readRDS(paste0(dir, model, "/", folder, "/", learned[j]))
      if (folder %in% c("mml_cpt", "mml_nb", "mml_rand")) mbl = symmetry_correction(vars, mbl, "AND")
      for (k in 1:length(mbt)) {
        ind = which(m[,1] == length(mbt[[k]]))
        ll[[ind]] = c(ll[[ind]], mb_false_finding(mbt[[k]], mbl[[k]]))
        # m[ind, ii + 1] = m[ind, ii + 1] + mb_false_finding(mbt[[k]], mbl[[k]])
        # w[ind, ii + 1] = w[ind, ii + 1] + 1
      }
      #jj = jj + 1
    }
    #write.table(edmtx, "~/Documents/Experiments/kdd_exp/kdd_results.csv", sep = ",", append = T, col.names = F, row.names = F)
    #ll[[i]] = edmtx
  }
  m[, ii + 1] = sapply(ll, mean)
  w[, ii + 1] = 1.96 * sapply(ll, sd) / sqrt(sapply(ll, length))
  
}

m = round(m, 1)
w = round(w, 1)
colnames(m) = colnames(w) = c("nmb", "MML_CPT", "MML_NB", "MML_RANDOM", "IAMB", "PCMB", "SLL")
m = data.frame(m)
m_melt = melt(m, id = "nmb")
colnames(m_melt)[2] = "Algorithm"
error = as.vector(w[, -1])
figure = ggplot(m_melt, aes(x = nmb, y = value, group = Algorithm, colour = Algorithm)) + 
  ylab(label = "Edit distance") + xlab("Markov blanket size") + geom_line(aes(linetype = Algorithm)) + geom_point(aes(shape = Algorithm)) + 
  ylim(0, 25) + xlim(0, max(mbt_len_mtx)) + guides(linetype = guide_legend()) +
  theme(legend.key.width = unit(1.5, "cm")) +
  geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
#figure
filename = paste0("ed_vs_mbsize_", nvars, "_5_4_1_", n, collapse = "")
filename = paste0(filename, ".pdf", collapse = "")
ggsave(filename, device = "pdf", path = "~/Documents/Journal KDD/figures/")

}
```

## Sym enforcement using mml (no better than deterministic sym enforcement)
```{r}
m = matrix(0, 10, 3)
for (k in 1:10) {
dag = randDag(10, 3)
cpts = randCPTs(dag, 4, 1)
data = rbn(cpts, 500)
vars = names(data)
n = nrow(data)
arities = sapply(data, nlevels)
names(arities) = c()
varCnt = count_occurance(data, arities)
data = data.matrix(data)
mbt = lapply(vars, bnlearn::mb, x = dag)
mbl = lapply(vars, forward_greedy_fast, data = data, varCnt = varCnt, arities = arities, vars = vars, sampleSize = n)
mbDiscrepancy = mb_discrepancy(vars, mbl)
if (nrow(mbDiscrepancy) > 0) {
  # cat("dis exist! \n")
  mbl_sym = sym_enforcement_mml(vars, varCnt, arities, n, mbl, mbDiscrepancy)
  mbl_sym2 = symmetry_correction(vars, mbl, "AND")
  #mb_discrepancy(vars, mbl_sym)
  ed = ed_sym = ed_sym2 = c()
  for (i in 1:ncol(data)) {
    ed = c(ed, mb_false_finding(mbt[[i]], mbl[[i]]))
    ed_sym = c(ed_sym, mb_false_finding(mbt[[i]], mbl_sym[[i]]))
    ed_sym2 = c(ed_sym2, mb_false_finding(mbt[[i]], mbl_sym2[[i]]))
  }
  # cat(mean(ed), "\n")
  # cat(mean(ed_sym), "\n")
  # cat(mean(ed_sym2), "\n")
}
m[k, ] = c(mean(ed), mean(ed_sym), mean(ed_sym2))
}

```


```{r}
model = "child"
if (model == "30_5_4_1") {
  # 30_5_4_1
  x = c(4.5,6.3,4.6,6.2,5.7,6.3,3.3,5.1,3.5,5.2,3.5,4.4,2.6,4.5,3,4.7,2.6,2.9)
  error = c(0.3,0.3,0.3,0.3,0.3,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.2,0.2,0.3,0.2,0.2)
} else if (model == "50_5_4_1") {
  # 50_5_4_1
  x = c(6.4,8.4,6.5,8.2,7.5,8,5.1,6.9,5.2,7.2,5.5,6.2,4.2,6.1,4.5,6.5,4.4,4.4)
  error = c(0.3,0.3,0.3,0.3,0.3,0.3,0.2,0.3,0.2,0.3,0.2,0.3,0.2,0.2,0.2,0.3,0.2,0.2)
} else if (model == "child") {
  # child
  x = c(0.9, 1.6, 1.4, 1.7, 1.4, 1, 0.7, 1.3, 0.9, 1.6, 1, 0.8, 0.5, 0.8, 0.2, 1.4, 0.2, 0.2)
  error = c(0.2, 0.2, 0.2,0.2,0.2,0.2, 0.1,0.2,0.2,0.2,0.2,0.1, 0.1,0.1,0.1,0.2,0.1,0.1)
} else if (model == "insurance") {
  # insurance
  x = c(3.3,4.7,3.7,3.6,3.4,3.1,2.9,4.4,3.1,3.7,3,2.7,2.1,3.8,2.4,2.8,1.8,2)
  error = c(0.2,0.3,0.3,0.3,0.2,0.2,0.2,0.3,0.3,0.3,0.2,0.2,0.2,0.3,0.2,0.2,0.2,0.2)
} else if (model == "alarm") {
  # alarm
  x = c(1.4,5.2,2.3,2.2,1.7,0.8,1,4.5,1.9,2,1.1,0.6,0.5,3.6,1.5,1.5,0.2,0.2)
  error = c(.1,.3,.2,.2,.2,.1,.1,.2,.2,.2,.1,.1,.1,.2,.1,.2,.1,0)
} else if (model == "barley") {
  # barley
  x = c(4,4.4,4.4,5.2,5.2,4.2,3.7,3.9,4.2,5.2,5.2,3.8,3.4,3.9,3.5,5.2,5.2,3.1)
  error = c(.3,.3,.3,.2,.2,.2,.3,.3,.3,.2,.2,.2,.3,.3,.3,.2,.2,.2)
} else if (model == "hailfinder") {
  # hailfinder
  x = c(4.4,6,5.2,3.5,3.5,4.3,4.4,5.9,5,3.5,3.5,4.1,4.3,6,5.1,3.5,3.5,4)
  error = c(.3,.3,.3,.2,.2,.3,.3,.3,.3,.2,.2,.3,.3,.3,.3,.2,.2,.3)
}

m = matrix(x, 3, 6, byrow = T)
if (model %in% c("30_5_4_1", "50_5_4_1")) {
  m = cbind(c(500, 2000, 5000), m)  
} else {
  m = cbind(c(500, 1000, 5000), m)
}

colnames(m) = c("nmb", "MML_CPT", "MML_NB", "MML_RANDOM", "IAMB", "PCMB", "SLL")
m = data.frame(m)
m_melt = melt(m, id = "nmb")
colnames(m_melt)[2] = "Algorithm"

figure = ggplot(m_melt, aes(x = nmb, y = value, group = Algorithm, colour = Algorithm)) + 
  ylab(label = "Edit distance") + xlab("Sample size") + geom_line(aes(linetype = Algorithm)) + geom_point(aes(shape = Algorithm)) + 
  ylim(0, 10) + xlim(500, 5000) + guides(linetype = guide_legend()) +
  theme(legend.key.width = unit(1.5, "cm")) +
  geom_errorbar(aes(ymin = value - error, ymax = value + error), width = 0.03)
figure
filename = paste0("ed_vs_samplesize_", model, ".pdf", collapse = "")
ggsave(filename, device = "pdf", path = "~/Documents/Journal KDD/figures/")




```


## MML w/ wrong prior
```{r}
dag = randDag(5, 2)
cpts = randCPTs(dag, 2, 1)
n = 500
data = rbn(cpts, n)
vars = colnames(data)
nvars = length(vars)
#data_cat = numeric2categorical(data)
arities = sapply(data, nlevels)
varCnt = di = count_occurance(data, arities)
#data = data.matrix(data)
target = "V1"
targetIndex = which(vars == target)
bnlearn::mb(dag, target)
forward_greedy(data, arities, vars, n, target, "cpt", varCnt = di, debug = T)

alpha_est = list()
ls = list()
for (i in 1:nvars) {
  ls[[i]] = freqs(table(data_cat[, i]))
}
k = 1
for (i in unique(arities)) {
  par = data.frame()
  ind = which(arities == i)
  if (length(ind) > 0) {
    for (j in ind) {
      par = rbind(par, ls[[j]])
    }
    alpha_est[[k]] = dirichlet.mle(par)$xsi
  }
  k = k + 1
}

alpha = which(sapply(alpha_est, length) == arities[targetIndex])
forward_greedy_fast(data, di, arities, vars, n, target, alpha=1, debug = F)
```













