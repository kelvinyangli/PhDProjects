\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pearl1988probabilistic}
\citation{koller1996toward}
\citation{cooper1997evaluation}
\citation{cheng2001kdd}
\citation{aliferis2010localb}
\newlabel{sec:abst}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{margaritis1999bayesian}
\citation{tsamardinos2003algorithms}
\citation{aliferis2003hiton}
\citation{tsamardinos2003time}
\citation{pena2007towards}
\citation{fu2008fast}
\citation{aliferis2010locala}
\citation{demorais2010novel}
\citation{liu2016swamping}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}}
\newlabel{sec:related}{{2}{2}{Related work}{section.2}{}}
\citation{cooper1997evaluation}
\citation{madden2002new}
\citation{acid2013score}
\citation{niinimaki2012local}
\citation{gao2017efficient}
\citation{frey2003identifying}
\citation{li2004}
\citation{strobl2016markov}
\@writefile{toc}{\contentsline {section}{\numberline {3}Markov blanket}{4}{section.3}}
\newlabel{sec:mb}{{3}{4}{Markov blanket}{section.3}{}}
\newlabel{def:markov}{{1}{4}{Markov blanket}{definition.1}{}}
\newlabel{def:bn}{{2}{4}{Markov blanket}{definition.2}{}}
\newlabel{def:entail}{{3}{4}{Markov blanket}{definition.3}{}}
\newlabel{def:imap}{{4}{4}{Markov blanket}{definition.4}{}}
\citation{wallace1968}
\newlabel{def:faithful}{{5}{5}{Markov blanket}{definition.5}{}}
\newlabel{def:equivalent}{{6}{5}{Markov blanket}{definition.6}{}}
\newlabel{def:mb}{{7}{5}{Markov blanket}{definition.7}{}}
\newlabel{prop:sym}{{1}{5}{Markov blanket}{proposition.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Minimum message length}{5}{section.4}}
\newlabel{sec:mml}{{4}{5}{Minimum message length}{section.4}{}}
\citation{wallace1987}
\citation{wallace2005}
\newlabel{eq:mml}{{1}{6}{Minimum message length}{equation.4.1}{}}
\newlabel{eq:mml_1}{{2}{6}{Minimum message length}{equation.4.2}{}}
\newlabel{eq:mml_2}{{3}{6}{Minimum message length}{equation.4.3}{}}
\citation{boulton1969information}
\newlabel{eq:msmml}{{4}{7}{Minimum message length}{equation.4.4}{}}
\newlabel{def:decomp}{{8}{7}{Minimum message length}{definition.8}{}}
\citation{geiger2001stratified}
\citation{haughton1988choice}
\citation{haughton1988choice}
\citation{geiger2001stratified}
\citation{geiger2001stratified}
\citation{haughton1988choice}
\citation{haughton1988choice}
\newlabel{def:consistent}{{9}{8}{Minimum message length}{definition.9}{}}
\newlabel{def:local_consistent}{{10}{9}{Minimum message length}{definition.10}{}}
\citation{neil1999learning}
\@writefile{toc}{\contentsline {section}{\numberline {5}LEARNING Markov blanket using MML}{10}{section.5}}
\newlabel{sec:mbmml}{{5}{10}{LEARNING Markov blanket using MML}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}MML for conditional probability table model}{10}{subsection.5.1}}
\newlabel{sec:mml_cpt}{{5.1}{10}{MML for conditional probability table model}{subsection.5.1}{}}
\newlabel{eq:mmlcpt}{{5}{11}{MML for conditional probability table model}{equation.5.5}{}}
\newlabel{prop:mmlcpt}{{4}{11}{MML for conditional probability table model}{proposition.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}MML for Na\"ive Bayes model}{12}{subsection.5.2}}
\newlabel{eq:mmlnb}{{7}{12}{MML for Na\"ive Bayes model}{equation.5.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}MML for Markov blanket polytree model}{13}{subsection.5.3}}
\newlabel{prop:nmbps}{{5}{14}{MML for Markov blanket polytree model}{proposition.5}{}}
\newlabel{eq:nmbps}{{8}{14}{MML for Markov blanket polytree model}{equation.5.8}{}}
\newlabel{enum_m0}{{9}{14}{MML for Markov blanket polytree model}{equation.5.9}{}}
\newlabel{enum_m1_branch}{{10}{14}{MML for Markov blanket polytree model}{equation.5.10}{}}
\newlabel{enum_m1}{{11}{15}{MML for Markov blanket polytree model}{equation.5.11}{}}
\newlabel{enum_gnmk}{{12}{15}{MML for Markov blanket polytree model}{equation.5.12}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{enum_dup_a}{{1a}{16}{2a\relax }{figure.caption.1}{}}
\newlabel{sub@enum_dup_a}{{a}{16}{2a\relax }{figure.caption.1}{}}
\newlabel{enum_dup_b}{{1b}{16}{2b\relax }{figure.caption.1}{}}
\newlabel{sub@enum_dup_b}{{b}{16}{2b\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of two duplicated Markov blanket polytrees when enumerating.\relax }}{16}{figure.caption.1}}
\newlabel{enum_duplicated}{{1}{16}{Example of two duplicated Markov blanket polytrees when enumerating.\relax }{figure.caption.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The number of labelled DAGs and MBPs on $n \in [0, 7]$ nodes.\relax }}{16}{table.caption.2}}
\newlabel{tb:nmbps}{{1}{16}{The number of labelled DAGs and MBPs on $n \in [0, 7]$ nodes.\relax }{table.caption.2}{}}
\newlabel{eq:mmlmbp}{{13}{16}{MML for Markov blanket polytree model}{equation.5.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Pseudo-code of the MBMML algorithm}{17}{subsection.5.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces MB discovery using MBMML+CPT/NB\relax }}{17}{algorithm.1}}
\newlabel{alg:mbmmlf}{{1}{17}{MB discovery using MBMML+CPT/NB\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces MB discovery using MBMML+ENSEMBLE\relax }}{18}{algorithm.2}}
\newlabel{alg:mbmmlr}{{2}{18}{MB discovery using MBMML+ENSEMBLE\relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Symmetry enforcement\relax }}{19}{algorithm.3}}
\newlabel{alg:sym}{{3}{19}{Symmetry enforcement\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments on Markov blanket discovery}{19}{section.6}}
\citation{pena2007towards}
\citation{niinimaki2012local}
\citation{chickering2002learning}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of tested Bayesian networks. 30-5-4-1 and 30-5-4-1 refer to artificial networks with 30 and 50 variables, maximum fan-in 5, maximum number of states 4, and uniform ($\ensuremath  {\mathchoice {\unhbox \voidb@x \hbox {\relax \mathweight  {bold}$\displaystyle \mathbf  {\alpha }$}} {\unhbox \voidb@x \hbox {\relax \mathweight  {bold}$\textstyle \mathbf  {\alpha }$}} {\unhbox \voidb@x \hbox {\relax \mathweight  {bold}$\scriptstyle \mathbf  {\alpha }$}} {\unhbox \voidb@x \hbox {\relax \mathweight  {bold}$\scriptscriptstyle \mathbf  {\alpha }$}}}=<1>$) parameter prior.\relax }}{21}{table.caption.3}}
\newlabel{tab:exp_models}{{2}{21}{Summary of tested Bayesian networks. 30-5-4-1 and 30-5-4-1 refer to artificial networks with 30 and 50 variables, maximum fan-in 5, maximum number of states 4, and uniform ($\vec {\alpha }=<1>$) parameter prior.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Accuracy on real models}{21}{subsection.6.1}}
\newlabel{sec:exp_real}{{6.1}{21}{Accuracy on real models}{subsection.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on CHILD network.\relax }}{22}{figure.caption.4}}
\newlabel{fg:child}{{2}{22}{Edit distance (with $95\%$ confidence intervals) v.s. sample size on CHILD network.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on BARLEY network. PCMB failed under 1000 and 5000 samples possibly due to an implementation error.\relax }}{22}{figure.caption.5}}
\newlabel{fg:barley}{{3}{22}{Edit distance (with $95\%$ confidence intervals) v.s. sample size on BARLEY network. PCMB failed under 1000 and 5000 samples possibly due to an implementation error.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Accuracy on artificial models}{23}{subsection.6.2}}
\newlabel{sec:exp_artificial}{{6.2}{23}{Accuracy on artificial models}{subsection.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on artificial Bayesian networks (30-5-4-1) containing 30 variables, maximum 5 parents and maximum 4 states for each variable.\relax }}{24}{figure.caption.6}}
\newlabel{fg:30}{{4}{24}{Edit distance (with $95\%$ confidence intervals) v.s. sample size on artificial Bayesian networks (30-5-4-1) containing 30 variables, maximum 5 parents and maximum 4 states for each variable.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on artificial Bayesian networks (50-5-4-1) containing 50 variables, maximum 5 parents and maximum 4 states for each variable.\relax }}{24}{figure.caption.7}}
\newlabel{fg:50}{{5}{24}{Edit distance (with $95\%$ confidence intervals) v.s. sample size on artificial Bayesian networks (50-5-4-1) containing 50 variables, maximum 5 parents and maximum 4 states for each variable.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Edit distance against Markov blanket size on 50-5-4-1 models with 500 samples.\relax }}{25}{figure.caption.8}}
\newlabel{fg:ed_mb_50_500}{{6}{25}{Edit distance against Markov blanket size on 50-5-4-1 models with 500 samples.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Edit distance against Markov blanket size on 50-5-4-1 models with 5000 samples.\relax }}{26}{figure.caption.9}}
\newlabel{fg:ed_mb_50_5000}{{7}{26}{Edit distance against Markov blanket size on 50-5-4-1 models with 5000 samples.\relax }{figure.caption.9}{}}
\citation{silander2007sensitivity}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces MBMML+CPT's edit distances using the true prior and uniform prior on a 30-5-4-1 model with 500 samples. The X-axis is the natural log scale of the true symmetric Dirichlet concentration parameter $\alpha = \{0.1, 0.4, 0.7, 1, 10, 40, 70, 100\}$.\relax }}{28}{figure.caption.10}}
\newlabel{fg:wrong_prior_500}{{8}{28}{MBMML+CPT's edit distances using the true prior and uniform prior on a 30-5-4-1 model with 500 samples. The X-axis is the natural log scale of the true symmetric Dirichlet concentration parameter $\alpha = \{0.1, 0.4, 0.7, 1, 10, 40, 70, 100\}$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces MBMML+CPT's edit distances using the true prior and uniform prior on a 30-5-4-1 model with 5000 samples. The X-axis is the natural log scale of the true symmetric Dirichlet concentration parameter $\alpha = \{0.1, 0.4, 0.7, 1, 10, 40, 70, 100\}$.\relax }}{28}{figure.caption.11}}
\newlabel{fg:wrong_prior_5000}{{9}{28}{MBMML+CPT's edit distances using the true prior and uniform prior on a 30-5-4-1 model with 5000 samples. The X-axis is the natural log scale of the true symmetric Dirichlet concentration parameter $\alpha = \{0.1, 0.4, 0.7, 1, 10, 40, 70, 100\}$.\relax }{figure.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Algorithm's computational complexity in big O notation.\relax }}{29}{table.caption.12}}
\newlabel{tb:bigo}{{3}{29}{Algorithm's computational complexity in big O notation.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Algorithm complexity}{29}{subsection.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{30}{section.7}}
\newlabel{sec:disc}{{7}{30}{Conclusion}{section.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Summary of edit distance (with $95\%$ confidence intervals) of all Markov blanket discovery algorithms on both real and artificial Bayesian networks. The best results are highlighted in grey. In real networks, SLL wins most of the times followed by MBMML+CPT, MBMML+NB, PCMB, MMLL+ENSEMBLE, IAMB. PCMB failed to learn on BARLEY networks under 1000 and 5000 samples possibly due to an implementation error. In artificial networks, MBMML+CPT and MBMML+ENSEMBLE win most of the times followed by SLL, PCMB, MBMML+NB, IAMB.\relax }}{31}{table.caption.13}}
\newlabel{tb:all_ed}{{4}{31}{Summary of edit distance (with $95\%$ confidence intervals) of all Markov blanket discovery algorithms on both real and artificial Bayesian networks. The best results are highlighted in grey. In real networks, SLL wins most of the times followed by MBMML+CPT, MBMML+NB, PCMB, MMLL+ENSEMBLE, IAMB. PCMB failed to learn on BARLEY networks under 1000 and 5000 samples possibly due to an implementation error. In artificial networks, MBMML+CPT and MBMML+ENSEMBLE win most of the times followed by SLL, PCMB, MBMML+NB, IAMB.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}appendix}{31}{section.8}}
\newlabel{tb:all_pre_rec}{{8}{33}{appendix}{table.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on INSURANCE network.\relax }}{34}{figure.caption.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on ALARM network.\relax }}{34}{figure.caption.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Edit distance (with $95\%$ confidence intervals) v.s. sample size on HAILFINDER network.\relax }}{35}{figure.caption.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Edit distance against Markov blanket size on 30-5-4-1 models with 100 samples.\relax }}{35}{figure.caption.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Edit distance against Markov blanket size on 30-5-4-1 models with 500 samples.\relax }}{36}{figure.caption.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Edit distance against Markov blanket size on 30-5-4-1 models with 2000 samples.\relax }}{36}{figure.caption.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Edit distance against Markov blanket size on 30-5-4-1 models with 5000 samples.\relax }}{37}{figure.caption.20}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Edit distance against Markov blanket size on 50-5-4-1 models with 100 samples.\relax }}{37}{figure.caption.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Edit distance against Markov blanket size on 50-5-4-1 models with 2000 samples.\relax }}{38}{figure.caption.22}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Notations\relax }}{39}{table.caption.23}}
\newlabel{my-label}{{5}{39}{Notations\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Notations}{39}{subsection.8.1}}
\bibstyle{named}
\bibdata{/home/kl/Documents/causal_discovery_ref_list}
\bibcite{acid2013score}{\citeauthoryear {Acid \bgroup \em  et al.\egroup }{2013}}
\bibcite{aliferis2003hiton}{\citeauthoryear {Aliferis \bgroup \em  et al.\egroup }{2003}}
\bibcite{aliferis2010locala}{\citeauthoryear {Aliferis \bgroup \em  et al.\egroup }{2010a}}
\bibcite{aliferis2010localb}{\citeauthoryear {Aliferis \bgroup \em  et al.\egroup }{2010b}}
\bibcite{boulton1969information}{\citeauthoryear {Boulton and Wallace}{1969}}
\bibcite{cheng2001kdd}{\citeauthoryear {Cheng \bgroup \em  et al.\egroup }{2001}}
\bibcite{chickering2002learning}{\citeauthoryear {Chickering}{2002}}
\bibcite{cooper1997evaluation}{\citeauthoryear {Cooper \bgroup \em  et al.\egroup }{1997}}
\bibcite{demorais2010novel}{\citeauthoryear {de Morais and Aussem}{2010}}
\bibcite{frey2003identifying}{\citeauthoryear {Frey \bgroup \em  et al.\egroup }{2003}}
\bibcite{fu2008fast}{\citeauthoryear {Fu and Desmarais}{2008}}
\bibcite{gao2017efficient}{\citeauthoryear {Gao and Ji}{2017}}
\bibcite{geiger2001stratified}{\citeauthoryear {Geiger \bgroup \em  et al.\egroup }{2001}}
\bibcite{haughton1988choice}{\citeauthoryear {Haughton}{1988}}
\bibcite{koller1996toward}{\citeauthoryear {Koller and Sahami}{1996}}
\bibcite{li2004}{\citeauthoryear {Li \bgroup \em  et al.\egroup }{2004}}
\bibcite{liu2016swamping}{\citeauthoryear {Liu and Liu}{2016}}
\bibcite{madden2002new}{\citeauthoryear {Madden}{2002}}
\bibcite{margaritis1999bayesian}{\citeauthoryear {Margaritis and Thrun}{1999}}
\bibcite{neil1999learning}{\citeauthoryear {Neil \bgroup \em  et al.\egroup }{1999}}
\bibcite{niinimaki2012local}{\citeauthoryear {Niinimaki and Parviainen}{2012}}
\bibcite{pearl1988probabilistic}{\citeauthoryear {Pearl}{1988}}
\bibcite{pena2007towards}{\citeauthoryear {Pe{\~n}a \bgroup \em  et al.\egroup }{2007}}
\bibcite{silander2007sensitivity}{\citeauthoryear {Silander \bgroup \em  et al.\egroup }{2007}}
\bibcite{strobl2016markov}{\citeauthoryear {Strobl and Visweswaran}{2016}}
\bibcite{tsamardinos2003time}{\citeauthoryear {Tsamardinos \bgroup \em  et al.\egroup }{2003a}}
\bibcite{tsamardinos2003algorithms}{\citeauthoryear {Tsamardinos \bgroup \em  et al.\egroup }{2003b}}
\bibcite{wallace1968}{\citeauthoryear {Wallace and Boulton}{1968}}
\bibcite{wallace1987}{\citeauthoryear {Wallace and Freeman}{1987}}
\bibcite{wallace2005}{\citeauthoryear {Wallace}{2005}}
